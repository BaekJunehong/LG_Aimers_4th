{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "acdab431",
      "metadata": {
        "id": "acdab431"
      },
      "source": [
        "## 1. 데이터 확인"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b8341e8",
      "metadata": {
        "id": "2b8341e8"
      },
      "source": [
        "### 필수 라이브러리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a315cc58",
      "metadata": {
        "id": "a315cc58"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore') # 경고 메세지 무시"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "38ccd6b1",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train_origin = pd.read_csv(\"data/train.csv\") # 학습용 데이터\n",
        "df_test_origin = pd.read_csv(\"data/submission.csv\") # 테스트 데이터(제출파일의 데이터)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0b323c11",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 59299 entries, 0 to 59298\n",
            "Data columns (total 29 columns):\n",
            " #   Column                   Non-Null Count  Dtype  \n",
            "---  ------                   --------------  -----  \n",
            " 0   bant_submit              59299 non-null  float64\n",
            " 1   customer_country         58317 non-null  object \n",
            " 2   business_unit            59299 non-null  object \n",
            " 3   com_reg_ver_win_rate     14568 non-null  float64\n",
            " 4   customer_idx             59299 non-null  int64  \n",
            " 5   customer_type            15338 non-null  object \n",
            " 6   enterprise               59299 non-null  object \n",
            " 7   historical_existing_cnt  13756 non-null  float64\n",
            " 8   id_strategic_ver         3444 non-null   float64\n",
            " 9   it_strategic_ver         1121 non-null   float64\n",
            " 10  idit_strategic_ver       4565 non-null   float64\n",
            " 11  customer_job             40566 non-null  object \n",
            " 12  lead_desc_length         59299 non-null  int64  \n",
            " 13  inquiry_type             58358 non-null  object \n",
            " 14  product_category         39925 non-null  object \n",
            " 15  product_subcategory      9235 non-null   object \n",
            " 16  product_modelname        9229 non-null   object \n",
            " 17  customer_country.1       58317 non-null  object \n",
            " 18  customer_position        59299 non-null  object \n",
            " 19  response_corporate       59299 non-null  object \n",
            " 20  expected_timeline        28436 non-null  object \n",
            " 21  ver_cus                  59299 non-null  int64  \n",
            " 22  ver_pro                  59299 non-null  int64  \n",
            " 23  ver_win_rate_x           18417 non-null  float64\n",
            " 24  ver_win_ratio_per_bu     15304 non-null  float64\n",
            " 25  business_area            18417 non-null  object \n",
            " 26  business_subarea         5526 non-null   object \n",
            " 27  lead_owner               59299 non-null  int64  \n",
            " 28  is_converted             59299 non-null  bool   \n",
            "dtypes: bool(1), float64(8), int64(5), object(15)\n",
            "memory usage: 12.7+ MB\n"
          ]
        }
      ],
      "source": [
        "df_train_origin.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af526c13",
      "metadata": {
        "id": "af526c13"
      },
      "source": [
        "## 2. 데이터 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cc30f10",
      "metadata": {
        "id": "2cc30f10"
      },
      "source": [
        "### 각 변수별 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "596c0909",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train_process = pd.read_csv('data/Ch2/df_train.csv')\n",
        "df_test_process = pd.read_csv('data/Ch2/df_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e0437e17",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 59299 entries, 0 to 59298\n",
            "Data columns (total 29 columns):\n",
            " #   Column                   Non-Null Count  Dtype  \n",
            "---  ------                   --------------  -----  \n",
            " 0   bant_submit              59299 non-null  float64\n",
            " 1   customer_country         59299 non-null  object \n",
            " 2   business_unit            59299 non-null  object \n",
            " 3   com_reg_ver_win_rate     14568 non-null  float64\n",
            " 4   customer_idx             59299 non-null  int64  \n",
            " 5   customer_type            59299 non-null  object \n",
            " 6   enterprise               59299 non-null  object \n",
            " 7   historical_existing_cnt  59299 non-null  float64\n",
            " 8   id_strategic_ver         59299 non-null  float64\n",
            " 9   it_strategic_ver         59299 non-null  float64\n",
            " 10  idit_strategic_ver       59299 non-null  float64\n",
            " 11  customer_job             59299 non-null  object \n",
            " 12  lead_desc_length         59299 non-null  int64  \n",
            " 13  inquiry_type             59299 non-null  object \n",
            " 14  product_category         59299 non-null  object \n",
            " 15  product_subcategory      59299 non-null  object \n",
            " 16  product_modelname        59299 non-null  object \n",
            " 17  customer_country.1       59299 non-null  object \n",
            " 18  customer_position        59299 non-null  object \n",
            " 19  response_corporate       59299 non-null  object \n",
            " 20  expected_timeline        59299 non-null  object \n",
            " 21  ver_cus                  59299 non-null  int64  \n",
            " 22  ver_pro                  59299 non-null  int64  \n",
            " 23  ver_win_rate_x           59299 non-null  float64\n",
            " 24  ver_win_ratio_per_bu     59299 non-null  float64\n",
            " 25  business_area            59299 non-null  object \n",
            " 26  business_subarea         59299 non-null  object \n",
            " 27  lead_owner               59299 non-null  int64  \n",
            " 28  is_converted             59299 non-null  bool   \n",
            "dtypes: bool(1), float64(8), int64(5), object(15)\n",
            "memory usage: 12.7+ MB\n"
          ]
        }
      ],
      "source": [
        "df_train_process.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08efd9a3",
      "metadata": {},
      "source": [
        "## 3. 피처엔지니어링"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bd47e00",
      "metadata": {
        "id": "4bd47e00"
      },
      "source": [
        "### 레이블 인코딩"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b53d4d09",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train_encoded = pd.read_csv('data/Ch3/df_train.csv')\n",
        "df_test_encoded = pd.read_csv('data/Ch3/df_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ca62d87e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 59299 entries, 0 to 59298\n",
            "Data columns (total 24 columns):\n",
            " #   Column                    Non-Null Count  Dtype  \n",
            "---  ------                    --------------  -----  \n",
            " 0   bant_submit               59299 non-null  float64\n",
            " 1   customer_country          59299 non-null  int64  \n",
            " 2   business_unit             59299 non-null  int64  \n",
            " 3   com_reg_ver_win_rate      59299 non-null  float64\n",
            " 4   customer_idx              59299 non-null  int64  \n",
            " 5   customer_type             59299 non-null  int64  \n",
            " 6   enterprise                59299 non-null  int64  \n",
            " 7   historical_existing_cnt   59299 non-null  float64\n",
            " 8   customer_job              59299 non-null  int64  \n",
            " 9   lead_desc_length          59299 non-null  int64  \n",
            " 10  customer_country.1        59299 non-null  int64  \n",
            " 11  customer_position         59299 non-null  int64  \n",
            " 12  response_corporate        59299 non-null  int64  \n",
            " 13  expected_timeline         59299 non-null  int64  \n",
            " 14  lead_owner                59299 non-null  int64  \n",
            " 15  is_converted              59299 non-null  bool   \n",
            " 16  id_business_area          59299 non-null  float64\n",
            " 17  it_business_area          59299 non-null  float64\n",
            " 18  idit_business_area        59299 non-null  float64\n",
            " 19  ver_cus_business_area     59299 non-null  int64  \n",
            " 20  ver_pro_product_category  59299 non-null  int64  \n",
            " 21  ver_win_business_area     59299 non-null  float64\n",
            " 22  ver_ratio_business_area   59299 non-null  float64\n",
            " 23  category_modelname        59299 non-null  int64  \n",
            "dtypes: bool(1), float64(8), int64(15)\n",
            "memory usage: 10.5 MB\n"
          ]
        }
      ],
      "source": [
        "df_train_encoded.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79ecfa9b",
      "metadata": {
        "id": "79ecfa9b"
      },
      "source": [
        "## 4. 모델 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "549e5839",
      "metadata": {},
      "source": [
        "### 데이터 분할"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "260c49ec",
      "metadata": {},
      "source": [
        "학습, 검증 데이터 분리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7af1b057",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(\n",
        "    df_train_encoded.drop(\"is_converted\", axis=1),\n",
        "    df_train_encoded[\"is_converted\"],\n",
        "    test_size=0.2,\n",
        "    shuffle=True,\n",
        "    random_state=400,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "295c9479",
      "metadata": {
        "id": "295c9479"
      },
      "source": [
        "### 모델 라이브러리"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b97e189",
      "metadata": {
        "id": "2b97e189"
      },
      "source": [
        "#### 단일모델 기준으로 사용할수 있는 모델들의 라이브러리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b870e254",
      "metadata": {},
      "outputs": [],
      "source": [
        "# from catboost import CatBoostClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "# 보팅\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "# 스테킹\n",
        "from sklearn.ensemble import StackingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7caaf33e",
      "metadata": {},
      "outputs": [],
      "source": [
        "break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3caf257b",
      "metadata": {
        "id": "3caf257b"
      },
      "source": [
        "### 최적 하이퍼 파라미터 찾기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9bd19c1",
      "metadata": {
        "id": "c9bd19c1"
      },
      "source": [
        "#### optuna를 통한 최적의 파라미터 찾기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "926b9b02",
      "metadata": {},
      "source": [
        "Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "71289338",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-02-17 20:55:05,044] A new study created in memory with name: no-name-ba012bf8-a0a6-4cb2-840d-7020c281ceb5\n",
            "[W 2024-02-17 20:55:09,497] Trial 0 failed with parameters: {'n_estimators': 729, 'max_depth': 28, 'min_samples_split': 16, 'min_samples_leaf': 15, 'criterion': 'entropy'} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"C:\\Users\\HamIG\\AppData\\Local\\Temp\\ipykernel_50360\\4050503408.py\", line 32, in <lambda>\n",
            "    study.optimize(lambda trial: objectiveRF(trial, x_train, y_train, x_val, y_val), n_trials=50)\n",
            "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\HamIG\\AppData\\Local\\Temp\\ipykernel_50360\\4050503408.py\", line 23, in objectiveRF\n",
            "    model.fit(x_tr, y_tr)\n",
            "  File \"c:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1351, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 489, in fit\n",
            "    trees = Parallel(\n",
            "            ^^^^^^^^^\n",
            "  File \"c:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 67, in __call__\n",
            "    return super().__call__(iterable_with_config)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py\", line 1863, in __call__\n",
            "    return output if self.return_generator else list(output)\n",
            "                                                ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py\", line 1792, in _get_sequential_output\n",
            "    res = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 129, in __call__\n",
            "    return self.function(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 192, in _parallel_build_trees\n",
            "    tree._fit(\n",
            "  File \"c:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 472, in _fit\n",
            "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
            "KeyboardInterrupt\n",
            "[W 2024-02-17 20:55:09,536] Trial 0 failed with value None.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# 하이퍼 파라미터 튜닝\u001b[39;00m\n\u001b[0;32m     31\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m, sampler\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTPESampler(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m---> 32\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjectiveRF\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest trial: score \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mparams \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mvalue, study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mparams))\n",
            "File \u001b[1;32mc:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
            "File \u001b[1;32mc:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
            "File \u001b[1;32mc:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
            "Cell \u001b[1;32mIn[10], line 32\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# 하이퍼 파라미터 튜닝\u001b[39;00m\n\u001b[0;32m     31\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m, sampler\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTPESampler(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m---> 32\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[43mobjectiveRF\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest trial: score \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mparams \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mvalue, study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mparams))\n",
            "Cell \u001b[1;32mIn[10], line 23\u001b[0m, in \u001b[0;36mobjectiveRF\u001b[1;34m(trial, x_tr, y_tr, x_val, y_val)\u001b[0m\n\u001b[0;32m     12\u001b[0m criterion \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39msuggest_categorical(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcriterion\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgini\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(\n\u001b[0;32m     15\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39mn_estimators,\n\u001b[0;32m     16\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39mmax_depth,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     21\u001b[0m )\n\u001b[1;32m---> 23\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_val)\n\u001b[0;32m     25\u001b[0m score \u001b[38;5;241m=\u001b[39m f1_score(y_val, pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1349\u001b[0m     )\n\u001b[0;32m   1350\u001b[0m ):\n\u001b[1;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    481\u001b[0m ]\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
            "File \u001b[1;32mc:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
            "File \u001b[1;32mc:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
            "File \u001b[1;32mc:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    190\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 192\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    201\u001b[0m         X,\n\u001b[0;32m    202\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    206\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "def objectiveRF(trial, x_tr, y_tr, x_val, y_val):\n",
        "    n_estimators = trial.suggest_int('n_estimators', 400, 1000)\n",
        "    max_depth = trial.suggest_int('max_depth', 10, 35)\n",
        "    min_samples_split = trial.suggest_int('min_samples_split', 10, 20)\n",
        "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 8, 20)\n",
        "    criterion = trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"])\n",
        "\n",
        "    model = RandomForestClassifier(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        min_samples_split=min_samples_split,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        criterion=criterion,\n",
        "        random_state=0\n",
        "    )\n",
        "\n",
        "    model.fit(x_tr, y_tr)\n",
        "    pred = model.predict(x_val)\n",
        "    score = f1_score(y_val, pred, average=\"weighted\")\n",
        "    \n",
        "    return score\n",
        "\n",
        "\n",
        "# 하이퍼 파라미터 튜닝\n",
        "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=0))\n",
        "study.optimize(lambda trial: objectiveRF(trial, x_train, y_train, x_val, y_val), n_trials=50)\n",
        "\n",
        "print('Best trial: score {}, \\nparams {}'.format(study.best_trial.value, study.best_trial.params))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db2d2d36",
      "metadata": {},
      "source": [
        "LGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "038aa9b1",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-02-17 17:24:10,277] A new study created in memory with name: no-name-5e1786de-1c89-42be-88aa-05fa97c761ec\n",
            "[I 2024-02-17 17:24:12,914] Trial 0 finished with value: 0.9753345480842605 and parameters: {'num_leaves': 166, 'max_depth': 19, 'n_estimators': 643, 'learning_rate': 0.05903948646972072, 'min_child_samples': 24}. Best is trial 0 with value: 0.9753345480842605.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best trial: score 0.9753345480842605, \n",
            "params {'num_leaves': 166, 'max_depth': 19, 'n_estimators': 643, 'learning_rate': 0.05903948646972072, 'min_child_samples': 24}\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def objectiveLGBM(trial, x_tr, y_tr, x_val, y_val):\n",
        "    param = {\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 2, 300),\n",
        "        'max_depth': trial.suggest_int('max_depth', 2, 25),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n",
        "        'verbose' : -1,\n",
        "        'random_state': 0\n",
        "    }\n",
        "    \n",
        "    model = LGBMClassifier(**param)\n",
        "    model.fit(x_tr, y_tr)\n",
        "    pred = model.predict(x_val)\n",
        "    score = f1_score(y_val, pred, average=\"weighted\")\n",
        "    \n",
        "    return score\n",
        "\n",
        "\n",
        "# 하이퍼 파라미터 튜닝\n",
        "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=0))\n",
        "study.optimize(lambda trial: objectiveLGBM(trial, x_train, y_train, x_val, y_val), n_trials=1)\n",
        "\n",
        "print('Best trial: score {}, \\nparams {}'.format(study.best_trial.value, study.best_trial.params))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f73c1b0",
      "metadata": {},
      "source": [
        "LGBM_dart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f29a724c",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-02-18 13:11:33,910] A new study created in memory with name: no-name-7f3daecf-0bb9-4452-82cf-97d02256059a\n",
            "[I 2024-02-18 13:12:05,034] Trial 0 finished with value: 0.9769814502529511 and parameters: {'num_leaves': 269, 'max_depth': 25, 'n_estimators': 762, 'learning_rate': 0.06814182280978279, 'min_child_samples': 11}. Best is trial 0 with value: 0.9769814502529511.\n",
            "[I 2024-02-18 13:12:47,432] Trial 1 finished with value: 0.977150084317032 and parameters: {'num_leaves': 286, 'max_depth': 19, 'n_estimators': 935, 'learning_rate': 0.09745639323507206, 'min_child_samples': 11}. Best is trial 1 with value: 0.977150084317032.\n",
            "[I 2024-02-18 13:13:12,508] Trial 2 finished with value: 0.9772344013490725 and parameters: {'num_leaves': 313, 'max_depth': 21, 'n_estimators': 741, 'learning_rate': 0.09479176468048628, 'min_child_samples': 6}. Best is trial 2 with value: 0.9772344013490725.\n",
            "[I 2024-02-18 13:13:40,866] Trial 3 finished with value: 0.9776559865092749 and parameters: {'num_leaves': 185, 'max_depth': 10, 'n_estimators': 900, 'learning_rate': 0.08447097256648954, 'min_child_samples': 18}. Best is trial 3 with value: 0.9776559865092749.\n",
            "[I 2024-02-18 13:14:03,299] Trial 4 finished with value: 0.9765598650927487 and parameters: {'num_leaves': 347, 'max_depth': 26, 'n_estimators': 677, 'learning_rate': 0.08463704234005189, 'min_child_samples': 6}. Best is trial 3 with value: 0.9776559865092749.\n",
            "[I 2024-02-18 13:14:37,093] Trial 5 finished with value: 0.9774873524451939 and parameters: {'num_leaves': 285, 'max_depth': 13, 'n_estimators': 967, 'learning_rate': 0.06652938252250501, 'min_child_samples': 11}. Best is trial 3 with value: 0.9776559865092749.\n",
            "[I 2024-02-18 13:14:57,400] Trial 6 finished with value: 0.9772344013490725 and parameters: {'num_leaves': 217, 'max_depth': 26, 'n_estimators': 674, 'learning_rate': 0.0697903764208054, 'min_child_samples': 5}. Best is trial 3 with value: 0.9776559865092749.\n",
            "[I 2024-02-18 13:15:32,122] Trial 7 finished with value: 0.9775716694772344 and parameters: {'num_leaves': 281, 'max_depth': 22, 'n_estimators': 770, 'learning_rate': 0.09606236549602369, 'min_child_samples': 15}. Best is trial 3 with value: 0.9776559865092749.\n",
            "[I 2024-02-18 13:16:02,654] Trial 8 finished with value: 0.9763912310286678 and parameters: {'num_leaves': 235, 'max_depth': 19, 'n_estimators': 819, 'learning_rate': 0.03421578301404889, 'min_child_samples': 15}. Best is trial 3 with value: 0.9776559865092749.\n",
            "[I 2024-02-18 13:16:18,000] Trial 9 finished with value: 0.9758010118043845 and parameters: {'num_leaves': 291, 'max_depth': 14, 'n_estimators': 477, 'learning_rate': 0.05207998456469287, 'min_child_samples': 10}. Best is trial 3 with value: 0.9776559865092749.\n",
            "[I 2024-02-18 13:16:28,726] Trial 10 finished with value: 0.9741989881956156 and parameters: {'num_leaves': 173, 'max_depth': 10, 'n_estimators': 412, 'learning_rate': 0.08067657644064885, 'min_child_samples': 20}. Best is trial 3 with value: 0.9776559865092749.\n",
            "[I 2024-02-18 13:17:01,381] Trial 11 finished with value: 0.977318718381113 and parameters: {'num_leaves': 171, 'max_depth': 21, 'n_estimators': 870, 'learning_rate': 0.08427107427927477, 'min_child_samples': 17}. Best is trial 3 with value: 0.9776559865092749.\n",
            "[I 2024-02-18 13:17:22,907] Trial 12 finished with value: 0.9772344013490725 and parameters: {'num_leaves': 224, 'max_depth': 30, 'n_estimators': 600, 'learning_rate': 0.09848255117033712, 'min_child_samples': 16}. Best is trial 3 with value: 0.9776559865092749.\n",
            "[I 2024-02-18 13:18:01,342] Trial 13 finished with value: 0.977318718381113 and parameters: {'num_leaves': 249, 'max_depth': 16, 'n_estimators': 880, 'learning_rate': 0.07921356327460062, 'min_child_samples': 20}. Best is trial 3 with value: 0.9776559865092749.\n",
            "[I 2024-02-18 13:18:41,671] Trial 14 finished with value: 0.9766441821247892 and parameters: {'num_leaves': 199, 'max_depth': 10, 'n_estimators': 989, 'learning_rate': 0.04987636143744907, 'min_child_samples': 14}. Best is trial 3 with value: 0.9776559865092749.\n",
            "[I 2024-02-18 13:19:21,988] Trial 15 finished with value: 0.9774030354131534 and parameters: {'num_leaves': 314, 'max_depth': 23, 'n_estimators': 818, 'learning_rate': 0.08921026320851876, 'min_child_samples': 18}. Best is trial 3 with value: 0.9776559865092749.\n",
            "[I 2024-02-18 13:19:42,219] Trial 16 finished with value: 0.9780775716694773 and parameters: {'num_leaves': 196, 'max_depth': 17, 'n_estimators': 601, 'learning_rate': 0.0751755644993597, 'min_child_samples': 14}. Best is trial 16 with value: 0.9780775716694773.\n",
            "[I 2024-02-18 13:19:59,792] Trial 17 finished with value: 0.9762225969645868 and parameters: {'num_leaves': 199, 'max_depth': 16, 'n_estimators': 562, 'learning_rate': 0.054713989934737736, 'min_child_samples': 13}. Best is trial 16 with value: 0.9780775716694773.\n",
            "[I 2024-02-18 13:20:20,547] Trial 18 finished with value: 0.9763912310286678 and parameters: {'num_leaves': 191, 'max_depth': 13, 'n_estimators': 586, 'learning_rate': 0.07691966799772587, 'min_child_samples': 18}. Best is trial 16 with value: 0.9780775716694773.\n",
            "[I 2024-02-18 13:20:43,286] Trial 19 finished with value: 0.9766441821247892 and parameters: {'num_leaves': 219, 'max_depth': 17, 'n_estimators': 634, 'learning_rate': 0.05911663476924277, 'min_child_samples': 9}. Best is trial 16 with value: 0.9780775716694773.\n",
            "[I 2024-02-18 13:21:00,550] Trial 20 finished with value: 0.9761382799325464 and parameters: {'num_leaves': 252, 'max_depth': 11, 'n_estimators': 518, 'learning_rate': 0.07337431311833703, 'min_child_samples': 18}. Best is trial 16 with value: 0.9780775716694773.\n",
            "[I 2024-02-18 13:21:25,639] Trial 21 finished with value: 0.9768971332209107 and parameters: {'num_leaves': 194, 'max_depth': 23, 'n_estimators': 732, 'learning_rate': 0.0901680292644343, 'min_child_samples': 15}. Best is trial 16 with value: 0.9780775716694773.\n",
            "[I 2024-02-18 13:21:58,635] Trial 22 finished with value: 0.9768128161888702 and parameters: {'num_leaves': 271, 'max_depth': 17, 'n_estimators': 799, 'learning_rate': 0.09014481192159496, 'min_child_samples': 13}. Best is trial 16 with value: 0.9780775716694773.\n",
            "[I 2024-02-18 13:22:35,999] Trial 23 finished with value: 0.9769814502529511 and parameters: {'num_leaves': 238, 'max_depth': 23, 'n_estimators': 914, 'learning_rate': 0.08573487180205903, 'min_child_samples': 16}. Best is trial 16 with value: 0.9780775716694773.\n",
            "[I 2024-02-18 13:23:07,373] Trial 24 finished with value: 0.977150084317032 and parameters: {'num_leaves': 185, 'max_depth': 28, 'n_estimators': 859, 'learning_rate': 0.07592816185965015, 'min_child_samples': 14}. Best is trial 16 with value: 0.9780775716694773.\n",
            "[I 2024-02-18 13:23:27,618] Trial 25 finished with value: 0.9766441821247892 and parameters: {'num_leaves': 207, 'max_depth': 19, 'n_estimators': 635, 'learning_rate': 0.06070455458516906, 'min_child_samples': 19}. Best is trial 16 with value: 0.9780775716694773.\n",
            "[I 2024-02-18 13:23:58,844] Trial 26 finished with value: 0.9766441821247892 and parameters: {'num_leaves': 306, 'max_depth': 14, 'n_estimators': 777, 'learning_rate': 0.09300575129771513, 'min_child_samples': 16}. Best is trial 16 with value: 0.9780775716694773.\n",
            "[I 2024-02-18 13:24:18,236] Trial 27 finished with value: 0.9772344013490725 and parameters: {'num_leaves': 178, 'max_depth': 12, 'n_estimators': 705, 'learning_rate': 0.07168968967114227, 'min_child_samples': 17}. Best is trial 16 with value: 0.9780775716694773.\n",
            "[I 2024-02-18 13:24:33,113] Trial 28 finished with value: 0.9756323777403035 and parameters: {'num_leaves': 333, 'max_depth': 21, 'n_estimators': 544, 'learning_rate': 0.038047295252528125, 'min_child_samples': 14}. Best is trial 16 with value: 0.9780775716694773.\n",
            "[I 2024-02-18 13:25:05,146] Trial 29 finished with value: 0.9770657672849916 and parameters: {'num_leaves': 264, 'max_depth': 24, 'n_estimators': 919, 'learning_rate': 0.08128278130420075, 'min_child_samples': 12}. Best is trial 16 with value: 0.9780775716694773.\n",
            "[I 2024-02-18 13:25:29,775] Trial 30 finished with value: 0.9768971332209107 and parameters: {'num_leaves': 276, 'max_depth': 16, 'n_estimators': 770, 'learning_rate': 0.06403406572705594, 'min_child_samples': 8}. Best is trial 16 with value: 0.9780775716694773.\n",
            "[I 2024-02-18 13:26:05,476] Trial 31 finished with value: 0.9774873524451939 and parameters: {'num_leaves': 296, 'max_depth': 12, 'n_estimators': 995, 'learning_rate': 0.0646267438302391, 'min_child_samples': 11}. Best is trial 16 with value: 0.9780775716694773.\n",
            "[I 2024-02-18 13:26:38,852] Trial 32 finished with value: 0.9775716694772344 and parameters: {'num_leaves': 278, 'max_depth': 13, 'n_estimators': 947, 'learning_rate': 0.06942872680143314, 'min_child_samples': 12}. Best is trial 16 with value: 0.9780775716694773.\n",
            "[I 2024-02-18 13:27:12,506] Trial 33 finished with value: 0.9779932546374368 and parameters: {'num_leaves': 253, 'max_depth': 14, 'n_estimators': 953, 'learning_rate': 0.09539877966186923, 'min_child_samples': 12}. Best is trial 16 with value: 0.9780775716694773.\n",
            "[I 2024-02-18 13:27:42,264] Trial 34 finished with value: 0.9775716694772344 and parameters: {'num_leaves': 208, 'max_depth': 18, 'n_estimators': 902, 'learning_rate': 0.09593691575086488, 'min_child_samples': 15}. Best is trial 16 with value: 0.9780775716694773.\n",
            "[I 2024-02-18 13:28:14,262] Trial 35 finished with value: 0.9767284991568297 and parameters: {'num_leaves': 253, 'max_depth': 15, 'n_estimators': 834, 'learning_rate': 0.0983235836480824, 'min_child_samples': 13}. Best is trial 16 with value: 0.9780775716694773.\n",
            "[I 2024-02-18 13:28:55,559] Trial 36 finished with value: 0.977318718381113 and parameters: {'num_leaves': 241, 'max_depth': 20, 'n_estimators': 960, 'learning_rate': 0.0936297899944163, 'min_child_samples': 10}. Best is trial 16 with value: 0.9780775716694773.\n",
            "[I 2024-02-18 13:29:23,148] Trial 37 finished with value: 0.9777403035413154 and parameters: {'num_leaves': 228, 'max_depth': 11, 'n_estimators': 734, 'learning_rate': 0.08621504200287952, 'min_child_samples': 17}. Best is trial 16 with value: 0.9780775716694773.\n",
            "[I 2024-02-18 13:29:47,711] Trial 38 finished with value: 0.9774030354131534 and parameters: {'num_leaves': 228, 'max_depth': 11, 'n_estimators': 682, 'learning_rate': 0.08597437259499226, 'min_child_samples': 19}. Best is trial 16 with value: 0.9780775716694773.\n",
            "[I 2024-02-18 13:30:08,329] Trial 39 finished with value: 0.9764755480607082 and parameters: {'num_leaves': 208, 'max_depth': 14, 'n_estimators': 633, 'learning_rate': 0.0833742612545162, 'min_child_samples': 17}. Best is trial 16 with value: 0.9780775716694773.\n",
            "[I 2024-02-18 13:30:31,593] Trial 40 finished with value: 0.9772344013490725 and parameters: {'num_leaves': 180, 'max_depth': 11, 'n_estimators': 725, 'learning_rate': 0.08816753927920354, 'min_child_samples': 7}. Best is trial 16 with value: 0.9780775716694773.\n",
            "[I 2024-02-18 13:30:58,618] Trial 41 finished with value: 0.9776559865092749 and parameters: {'num_leaves': 229, 'max_depth': 10, 'n_estimators': 746, 'learning_rate': 0.09969182852313625, 'min_child_samples': 15}. Best is trial 16 with value: 0.9780775716694773.\n",
            "[I 2024-02-18 13:31:12,714] Trial 42 finished with value: 0.9772344013490725 and parameters: {'num_leaves': 232, 'max_depth': 10, 'n_estimators': 475, 'learning_rate': 0.0998445334895883, 'min_child_samples': 14}. Best is trial 16 with value: 0.9780775716694773.\n",
            "[I 2024-02-18 13:31:34,249] Trial 43 finished with value: 0.9781618887015177 and parameters: {'num_leaves': 217, 'max_depth': 12, 'n_estimators': 660, 'learning_rate': 0.09241556338481696, 'min_child_samples': 16}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:31:56,214] Trial 44 finished with value: 0.9774873524451939 and parameters: {'num_leaves': 217, 'max_depth': 12, 'n_estimators': 675, 'learning_rate': 0.09318037403676462, 'min_child_samples': 19}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:32:15,850] Trial 45 finished with value: 0.9762225969645868 and parameters: {'num_leaves': 246, 'max_depth': 15, 'n_estimators': 599, 'learning_rate': 0.08033290778576815, 'min_child_samples': 16}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:32:36,698] Trial 46 finished with value: 0.9779089376053963 and parameters: {'num_leaves': 213, 'max_depth': 13, 'n_estimators': 655, 'learning_rate': 0.07681644038481325, 'min_child_samples': 17}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:32:59,850] Trial 47 finished with value: 0.9774873524451939 and parameters: {'num_leaves': 216, 'max_depth': 15, 'n_estimators': 701, 'learning_rate': 0.07787613831678059, 'min_child_samples': 17}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:33:21,892] Trial 48 finished with value: 0.9769814502529511 and parameters: {'num_leaves': 263, 'max_depth': 13, 'n_estimators': 657, 'learning_rate': 0.07403692237604145, 'min_child_samples': 12}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:33:34,939] Trial 49 finished with value: 0.9766441821247892 and parameters: {'num_leaves': 201, 'max_depth': 14, 'n_estimators': 495, 'learning_rate': 0.08295223986272587, 'min_child_samples': 16}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:33:51,761] Trial 50 finished with value: 0.9781618887015177 and parameters: {'num_leaves': 223, 'max_depth': 12, 'n_estimators': 566, 'learning_rate': 0.08688477560623672, 'min_child_samples': 15}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:34:08,295] Trial 51 finished with value: 0.9768128161888702 and parameters: {'num_leaves': 223, 'max_depth': 12, 'n_estimators': 564, 'learning_rate': 0.08661123089892953, 'min_child_samples': 15}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:34:27,150] Trial 52 finished with value: 0.9779932546374368 and parameters: {'num_leaves': 213, 'max_depth': 11, 'n_estimators': 614, 'learning_rate': 0.09196153189676393, 'min_child_samples': 14}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:34:46,204] Trial 53 finished with value: 0.9775716694772344 and parameters: {'num_leaves': 214, 'max_depth': 13, 'n_estimators': 614, 'learning_rate': 0.09144627513054637, 'min_child_samples': 13}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:35:02,603] Trial 54 finished with value: 0.9776559865092749 and parameters: {'num_leaves': 192, 'max_depth': 14, 'n_estimators': 571, 'learning_rate': 0.09609385660290737, 'min_child_samples': 14}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:35:15,771] Trial 55 finished with value: 0.977318718381113 and parameters: {'num_leaves': 201, 'max_depth': 12, 'n_estimators': 521, 'learning_rate': 0.08882776033063139, 'min_child_samples': 11}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:35:34,026] Trial 56 finished with value: 0.9768971332209107 and parameters: {'num_leaves': 212, 'max_depth': 17, 'n_estimators': 655, 'learning_rate': 0.09173912561029023, 'min_child_samples': 14}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:35:49,487] Trial 57 finished with value: 0.9768971332209107 and parameters: {'num_leaves': 187, 'max_depth': 15, 'n_estimators': 606, 'learning_rate': 0.08239987901945582, 'min_child_samples': 15}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:35:58,785] Trial 58 finished with value: 0.9774873524451939 and parameters: {'num_leaves': 243, 'max_depth': 18, 'n_estimators': 426, 'learning_rate': 0.09521011406186562, 'min_child_samples': 13}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:36:12,287] Trial 59 finished with value: 0.977150084317032 and parameters: {'num_leaves': 222, 'max_depth': 13, 'n_estimators': 540, 'learning_rate': 0.07928165415894776, 'min_child_samples': 16}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:36:27,827] Trial 60 finished with value: 0.9774873524451939 and parameters: {'num_leaves': 234, 'max_depth': 11, 'n_estimators': 586, 'learning_rate': 0.0762020370788553, 'min_child_samples': 10}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:36:44,599] Trial 61 finished with value: 0.9776559865092749 and parameters: {'num_leaves': 204, 'max_depth': 11, 'n_estimators': 623, 'learning_rate': 0.08783845875213392, 'min_child_samples': 17}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:37:06,341] Trial 62 finished with value: 0.9769814502529511 and parameters: {'num_leaves': 225, 'max_depth': 11, 'n_estimators': 718, 'learning_rate': 0.08533010959385356, 'min_child_samples': 18}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:37:24,030] Trial 63 finished with value: 0.977318718381113 and parameters: {'num_leaves': 195, 'max_depth': 10, 'n_estimators': 653, 'learning_rate': 0.09116479759529396, 'min_child_samples': 16}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:37:45,422] Trial 64 finished with value: 0.977150084317032 and parameters: {'num_leaves': 253, 'max_depth': 12, 'n_estimators': 689, 'learning_rate': 0.09712049937753095, 'min_child_samples': 18}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:38:01,270] Trial 65 finished with value: 0.9768971332209107 and parameters: {'num_leaves': 237, 'max_depth': 14, 'n_estimators': 583, 'learning_rate': 0.07215548324607068, 'min_child_samples': 17}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:38:14,360] Trial 66 finished with value: 0.9766441821247892 and parameters: {'num_leaves': 212, 'max_depth': 13, 'n_estimators': 542, 'learning_rate': 0.06643897735971525, 'min_child_samples': 14}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:38:32,417] Trial 67 finished with value: 0.9777403035413154 and parameters: {'num_leaves': 222, 'max_depth': 12, 'n_estimators': 642, 'learning_rate': 0.09393436096565663, 'min_child_samples': 12}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:38:58,989] Trial 68 finished with value: 0.9774873524451939 and parameters: {'num_leaves': 258, 'max_depth': 16, 'n_estimators': 789, 'learning_rate': 0.08898359147491533, 'min_child_samples': 15}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:39:19,419] Trial 69 finished with value: 0.9776559865092749 and parameters: {'num_leaves': 230, 'max_depth': 10, 'n_estimators': 670, 'learning_rate': 0.08415046931963785, 'min_child_samples': 17}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:39:41,126] Trial 70 finished with value: 0.9778246205733558 and parameters: {'num_leaves': 208, 'max_depth': 11, 'n_estimators': 744, 'learning_rate': 0.08688744351275013, 'min_child_samples': 11}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:40:02,856] Trial 71 finished with value: 0.9775716694772344 and parameters: {'num_leaves': 207, 'max_depth': 11, 'n_estimators': 746, 'learning_rate': 0.08713383479572953, 'min_child_samples': 11}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:40:24,317] Trial 72 finished with value: 0.9778246205733558 and parameters: {'num_leaves': 195, 'max_depth': 13, 'n_estimators': 706, 'learning_rate': 0.0900654153342369, 'min_child_samples': 10}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:40:43,662] Trial 73 finished with value: 0.9779089376053963 and parameters: {'num_leaves': 198, 'max_depth': 14, 'n_estimators': 616, 'learning_rate': 0.0919466202999461, 'min_child_samples': 10}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:41:01,845] Trial 74 finished with value: 0.9775716694772344 and parameters: {'num_leaves': 178, 'max_depth': 16, 'n_estimators': 616, 'learning_rate': 0.09265182777785244, 'min_child_samples': 8}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:41:18,809] Trial 75 finished with value: 0.9756323777403035 and parameters: {'num_leaves': 187, 'max_depth': 15, 'n_estimators': 591, 'learning_rate': 0.04577823699278688, 'min_child_samples': 9}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:41:38,605] Trial 76 finished with value: 0.977318718381113 and parameters: {'num_leaves': 197, 'max_depth': 14, 'n_estimators': 627, 'learning_rate': 0.09705729175137005, 'min_child_samples': 9}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:42:01,584] Trial 77 finished with value: 0.9777403035413154 and parameters: {'num_leaves': 210, 'max_depth': 12, 'n_estimators': 558, 'learning_rate': 0.09452406349341184, 'min_child_samples': 11}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:42:29,791] Trial 78 finished with value: 0.9769814502529511 and parameters: {'num_leaves': 204, 'max_depth': 18, 'n_estimators': 644, 'learning_rate': 0.08099496717635739, 'min_child_samples': 12}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:42:57,288] Trial 79 finished with value: 0.9774873524451939 and parameters: {'num_leaves': 182, 'max_depth': 30, 'n_estimators': 663, 'learning_rate': 0.06912077906758367, 'min_child_samples': 13}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:43:36,073] Trial 80 finished with value: 0.9772344013490725 and parameters: {'num_leaves': 219, 'max_depth': 13, 'n_estimators': 841, 'learning_rate': 0.06271042133295432, 'min_child_samples': 10}. Best is trial 43 with value: 0.9781618887015177.\n",
            "[I 2024-02-18 13:44:03,360] Trial 81 finished with value: 0.9784148397976391 and parameters: {'num_leaves': 170, 'max_depth': 13, 'n_estimators': 692, 'learning_rate': 0.0916736042020453, 'min_child_samples': 10}. Best is trial 81 with value: 0.9784148397976391.\n",
            "[I 2024-02-18 13:44:30,507] Trial 82 finished with value: 0.9770657672849916 and parameters: {'num_leaves': 170, 'max_depth': 12, 'n_estimators': 691, 'learning_rate': 0.09021179112627321, 'min_child_samples': 10}. Best is trial 81 with value: 0.9784148397976391.\n",
            "[I 2024-02-18 13:44:54,195] Trial 83 finished with value: 0.9776559865092749 and parameters: {'num_leaves': 188, 'max_depth': 14, 'n_estimators': 604, 'learning_rate': 0.09268729942812232, 'min_child_samples': 5}. Best is trial 81 with value: 0.9784148397976391.\n",
            "[I 2024-02-18 13:45:18,401] Trial 84 finished with value: 0.9772344013490725 and parameters: {'num_leaves': 174, 'max_depth': 13, 'n_estimators': 642, 'learning_rate': 0.09513426801694425, 'min_child_samples': 12}. Best is trial 81 with value: 0.9784148397976391.\n",
            "[I 2024-02-18 13:45:57,549] Trial 85 finished with value: 0.9774873524451939 and parameters: {'num_leaves': 204, 'max_depth': 10, 'n_estimators': 878, 'learning_rate': 0.08443052656596498, 'min_child_samples': 9}. Best is trial 81 with value: 0.9784148397976391.\n",
            "[I 2024-02-18 13:46:20,642] Trial 86 finished with value: 0.9772344013490725 and parameters: {'num_leaves': 191, 'max_depth': 28, 'n_estimators': 577, 'learning_rate': 0.09869439427327799, 'min_child_samples': 11}. Best is trial 81 with value: 0.9784148397976391.\n",
            "[I 2024-02-18 13:46:54,097] Trial 87 finished with value: 0.9775716694772344 and parameters: {'num_leaves': 217, 'max_depth': 11, 'n_estimators': 757, 'learning_rate': 0.0890543733877063, 'min_child_samples': 11}. Best is trial 81 with value: 0.9784148397976391.\n",
            "[I 2024-02-18 13:47:24,735] Trial 88 finished with value: 0.9770657672849916 and parameters: {'num_leaves': 199, 'max_depth': 20, 'n_estimators': 713, 'learning_rate': 0.0742539601550013, 'min_child_samples': 15}. Best is trial 81 with value: 0.9784148397976391.\n",
            "[I 2024-02-18 13:47:58,732] Trial 89 finished with value: 0.977318718381113 and parameters: {'num_leaves': 286, 'max_depth': 15, 'n_estimators': 676, 'learning_rate': 0.09114302428305182, 'min_child_samples': 8}. Best is trial 81 with value: 0.9784148397976391.\n",
            "[I 2024-02-18 13:48:20,999] Trial 90 finished with value: 0.9741989881956156 and parameters: {'num_leaves': 183, 'max_depth': 17, 'n_estimators': 619, 'learning_rate': 0.03041444707707658, 'min_child_samples': 14}. Best is trial 81 with value: 0.9784148397976391.\n",
            "[I 2024-02-18 13:48:48,989] Trial 91 finished with value: 0.9778246205733558 and parameters: {'num_leaves': 195, 'max_depth': 14, 'n_estimators': 698, 'learning_rate': 0.09033331062005914, 'min_child_samples': 10}. Best is trial 81 with value: 0.9784148397976391.\n",
            "[I 2024-02-18 13:49:02,486] Trial 92 finished with value: 0.976053962900506 and parameters: {'num_leaves': 212, 'max_depth': 13, 'n_estimators': 525, 'learning_rate': 0.05633501038658034, 'min_child_samples': 9}. Best is trial 81 with value: 0.9784148397976391.\n",
            "[I 2024-02-18 13:49:23,845] Trial 93 finished with value: 0.9777403035413154 and parameters: {'num_leaves': 175, 'max_depth': 12, 'n_estimators': 708, 'learning_rate': 0.08776957205300216, 'min_child_samples': 10}. Best is trial 81 with value: 0.9784148397976391.\n",
            "[I 2024-02-18 13:49:44,480] Trial 94 finished with value: 0.9770657672849916 and parameters: {'num_leaves': 205, 'max_depth': 13, 'n_estimators': 667, 'learning_rate': 0.09664312075617214, 'min_child_samples': 12}. Best is trial 81 with value: 0.9784148397976391.\n",
            "[I 2024-02-18 13:50:22,581] Trial 95 finished with value: 0.9774030354131534 and parameters: {'num_leaves': 271, 'max_depth': 11, 'n_estimators': 971, 'learning_rate': 0.07815901297365725, 'min_child_samples': 13}. Best is trial 81 with value: 0.9784148397976391.\n",
            "[I 2024-02-18 13:50:50,085] Trial 96 finished with value: 0.9765598650927487 and parameters: {'num_leaves': 201, 'max_depth': 12, 'n_estimators': 808, 'learning_rate': 0.09438449020360186, 'min_child_samples': 15}. Best is trial 81 with value: 0.9784148397976391.\n",
            "[I 2024-02-18 13:51:09,146] Trial 97 finished with value: 0.9764755480607082 and parameters: {'num_leaves': 335, 'max_depth': 15, 'n_estimators': 555, 'learning_rate': 0.08254632053957868, 'min_child_samples': 16}. Best is trial 81 with value: 0.9784148397976391.\n",
            "[I 2024-02-18 13:51:33,714] Trial 98 finished with value: 0.9774873524451939 and parameters: {'num_leaves': 226, 'max_depth': 13, 'n_estimators': 728, 'learning_rate': 0.09205834151620945, 'min_child_samples': 10}. Best is trial 81 with value: 0.9784148397976391.\n",
            "[I 2024-02-18 13:51:54,728] Trial 99 finished with value: 0.9775716694772344 and parameters: {'num_leaves': 209, 'max_depth': 14, 'n_estimators': 651, 'learning_rate': 0.08538298649449626, 'min_child_samples': 11}. Best is trial 81 with value: 0.9784148397976391.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best trial: score 0.9784148397976391, \n",
            "params {'num_leaves': 170, 'max_depth': 13, 'n_estimators': 692, 'learning_rate': 0.0916736042020453, 'min_child_samples': 10}\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def objectiveLGBM(trial, x_tr, y_tr, x_val, y_val):\n",
        "    param = {\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 170, 350),\n",
        "        'max_depth': trial.suggest_int('max_depth', 10, 30),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 400, 1000),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.03, 0.1),\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 20),\n",
        "        'verbose' : -1,\n",
        "        'boosting' : 'dart',\n",
        "        'random_state': 0\n",
        "    }\n",
        "    \n",
        "    model = LGBMClassifier(**param)\n",
        "    model.fit(x_tr, y_tr)\n",
        "    pred = model.predict(x_val)\n",
        "    score = f1_score(y_val, pred, average=\"micro\")\n",
        "    \n",
        "    return score\n",
        "\n",
        "\n",
        "# 하이퍼 파라미터 튜닝\n",
        "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=0))\n",
        "study.optimize(lambda trial: objectiveLGBM(trial, x_train, y_train, x_val, y_val), n_trials=100)\n",
        "\n",
        "print('Best trial: score {}, \\nparams {}'.format(study.best_trial.value, study.best_trial.params))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f42df31",
      "metadata": {},
      "source": [
        "XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "9067b7c9",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-02-17 22:01:44,818] A new study created in memory with name: no-name-f2ea9108-4035-4ae0-aaf6-0519e1ab98eb\n",
            "[I 2024-02-17 22:01:50,560] Trial 0 finished with value: 0.9741781813668445 and parameters: {'n_estimators': 2049, 'learning_rate': 0.08006325564606936, 'max_depth': 10, 'eval_metric': 'error'}. Best is trial 0 with value: 0.9741781813668445.\n",
            "[I 2024-02-17 22:01:56,881] Trial 1 finished with value: 0.9743102014356569 and parameters: {'n_estimators': 1938, 'learning_rate': 0.09242411005474559, 'max_depth': 15, 'eval_metric': 'auc'}. Best is trial 1 with value: 0.9743102014356569.\n",
            "[I 2024-02-17 22:01:59,313] Trial 2 finished with value: 0.9691065698382101 and parameters: {'n_estimators': 2068, 'learning_rate': 0.09479176468048628, 'max_depth': 3, 'eval_metric': 'error'}. Best is trial 1 with value: 0.9743102014356569.\n",
            "[I 2024-02-17 22:02:06,775] Trial 3 finished with value: 0.9742829019304714 and parameters: {'n_estimators': 2278, 'learning_rate': 0.09090085037727735, 'max_depth': 15, 'eval_metric': 'logloss'}. Best is trial 1 with value: 0.9743102014356569.\n",
            "[I 2024-02-17 22:02:09,027] Trial 4 finished with value: 0.9711694541336482 and parameters: {'n_estimators': 1618, 'learning_rate': 0.07479447149292667, 'max_depth': 4, 'eval_metric': 'logloss'}. Best is trial 1 with value: 0.9743102014356569.\n",
            "[I 2024-02-17 22:02:13,235] Trial 5 finished with value: 0.9739396800141163 and parameters: {'n_estimators': 1764, 'learning_rate': 0.08419635826039518, 'max_depth': 8, 'eval_metric': 'error'}. Best is trial 1 with value: 0.9743102014356569.\n",
            "[I 2024-02-17 22:02:19,801] Trial 6 finished with value: 0.9744014514060749 and parameters: {'n_estimators': 2112, 'learning_rate': 0.07318537978123299, 'max_depth': 15, 'eval_metric': 'logloss'}. Best is trial 6 with value: 0.9744014514060749.\n",
            "[I 2024-02-17 22:02:27,058] Trial 7 finished with value: 0.9744288329021991 and parameters: {'n_estimators': 2198, 'learning_rate': 0.03421578301404889, 'max_depth': 11, 'eval_metric': 'logloss'}. Best is trial 7 with value: 0.9744288329021991.\n",
            "[I 2024-02-17 22:02:32,318] Trial 8 finished with value: 0.9742848498152539 and parameters: {'n_estimators': 1815, 'learning_rate': 0.055459753965983585, 'max_depth': 10, 'eval_metric': 'auc'}. Best is trial 7 with value: 0.9744288329021991.\n",
            "[I 2024-02-17 22:02:37,660] Trial 9 finished with value: 0.9744014514060749 and parameters: {'n_estimators': 1709, 'learning_rate': 0.04129166625194974, 'max_depth': 11, 'eval_metric': 'auc'}. Best is trial 7 with value: 0.9744288329021991.\n",
            "[I 2024-02-17 22:02:43,464] Trial 10 finished with value: 0.973576116758472 and parameters: {'n_estimators': 2448, 'learning_rate': 0.031637483082472106, 'max_depth': 7, 'eval_metric': 'logloss'}. Best is trial 7 with value: 0.9744288329021991.\n",
            "[I 2024-02-17 22:02:51,236] Trial 11 finished with value: 0.9737633645957334 and parameters: {'n_estimators': 2235, 'learning_rate': 0.06354660666412854, 'max_depth': 13, 'eval_metric': 'logloss'}. Best is trial 7 with value: 0.9744288329021991.\n",
            "[I 2024-02-17 22:02:59,661] Trial 12 finished with value: 0.974576429763161 and parameters: {'n_estimators': 2240, 'learning_rate': 0.0565334216020282, 'max_depth': 13, 'eval_metric': 'logloss'}. Best is trial 12 with value: 0.974576429763161.\n",
            "[I 2024-02-17 22:03:08,366] Trial 13 finished with value: 0.9746821153186992 and parameters: {'n_estimators': 2500, 'learning_rate': 0.04950491716615289, 'max_depth': 12, 'eval_metric': 'logloss'}. Best is trial 13 with value: 0.9746821153186992.\n",
            "[I 2024-02-17 22:03:17,177] Trial 14 finished with value: 0.9739087413680063 and parameters: {'n_estimators': 2474, 'learning_rate': 0.05135230928797736, 'max_depth': 13, 'eval_metric': 'logloss'}. Best is trial 13 with value: 0.9746821153186992.\n",
            "[I 2024-02-17 22:03:26,866] Trial 15 finished with value: 0.9744831967121845 and parameters: {'n_estimators': 2361, 'learning_rate': 0.04971527320027113, 'max_depth': 13, 'eval_metric': 'logloss'}. Best is trial 13 with value: 0.9746821153186992.\n",
            "[I 2024-02-17 22:03:33,805] Trial 16 finished with value: 0.9742713194555568 and parameters: {'n_estimators': 2349, 'learning_rate': 0.06219154807932624, 'max_depth': 6, 'eval_metric': 'logloss'}. Best is trial 13 with value: 0.9746821153186992.\n",
            "[I 2024-02-17 22:03:42,778] Trial 17 finished with value: 0.9741202932707477 and parameters: {'n_estimators': 2482, 'learning_rate': 0.042916647446796546, 'max_depth': 12, 'eval_metric': 'logloss'}. Best is trial 13 with value: 0.9746821153186992.\n",
            "[I 2024-02-17 22:03:49,650] Trial 18 finished with value: 0.9742188594505418 and parameters: {'n_estimators': 2344, 'learning_rate': 0.05712671455467995, 'max_depth': 9, 'eval_metric': 'auc'}. Best is trial 13 with value: 0.9746821153186992.\n",
            "[I 2024-02-17 22:03:56,505] Trial 19 finished with value: 0.9739884746424992 and parameters: {'n_estimators': 1949, 'learning_rate': 0.06898301368356509, 'max_depth': 14, 'eval_metric': 'error'}. Best is trial 13 with value: 0.9746821153186992.\n",
            "[I 2024-02-17 22:04:05,969] Trial 20 finished with value: 0.9739642108826018 and parameters: {'n_estimators': 2156, 'learning_rate': 0.043445384216354375, 'max_depth': 12, 'eval_metric': 'logloss'}. Best is trial 13 with value: 0.9746821153186992.\n",
            "[I 2024-02-17 22:04:14,183] Trial 21 finished with value: 0.9742417023294213 and parameters: {'n_estimators': 2370, 'learning_rate': 0.049842451094995986, 'max_depth': 13, 'eval_metric': 'logloss'}. Best is trial 13 with value: 0.9746821153186992.\n",
            "[I 2024-02-17 22:04:23,010] Trial 22 finished with value: 0.9739642108826018 and parameters: {'n_estimators': 2389, 'learning_rate': 0.04733774321479736, 'max_depth': 12, 'eval_metric': 'logloss'}. Best is trial 13 with value: 0.9746821153186992.\n",
            "[I 2024-02-17 22:04:31,649] Trial 23 finished with value: 0.9746157449515813 and parameters: {'n_estimators': 2306, 'learning_rate': 0.05858521192249157, 'max_depth': 14, 'eval_metric': 'logloss'}. Best is trial 13 with value: 0.9746821153186992.\n",
            "[I 2024-02-17 22:04:40,054] Trial 24 finished with value: 0.9738984441092841 and parameters: {'n_estimators': 2278, 'learning_rate': 0.05842391349160403, 'max_depth': 14, 'eval_metric': 'logloss'}. Best is trial 13 with value: 0.9746821153186992.\n",
            "[I 2024-02-17 22:04:48,613] Trial 25 finished with value: 0.9740543524420896 and parameters: {'n_estimators': 2273, 'learning_rate': 0.03696659957752883, 'max_depth': 14, 'eval_metric': 'logloss'}. Best is trial 13 with value: 0.9746821153186992.\n",
            "[I 2024-02-17 22:04:56,849] Trial 26 finished with value: 0.9742713194555568 and parameters: {'n_estimators': 2498, 'learning_rate': 0.06408317022711205, 'max_depth': 11, 'eval_metric': 'logloss'}. Best is trial 13 with value: 0.9746821153186992.\n",
            "[I 2024-02-17 22:05:03,500] Trial 27 finished with value: 0.9739917437904003 and parameters: {'n_estimators': 2168, 'learning_rate': 0.05599940444336833, 'max_depth': 10, 'eval_metric': 'logloss'}. Best is trial 13 with value: 0.9746821153186992.\n",
            "[I 2024-02-17 22:05:12,120] Trial 28 finished with value: 0.9745886154293616 and parameters: {'n_estimators': 2409, 'learning_rate': 0.0692371280104711, 'max_depth': 14, 'eval_metric': 'error'}. Best is trial 13 with value: 0.9746821153186992.\n",
            "[I 2024-02-17 22:05:17,901] Trial 29 finished with value: 0.9742140677286236 and parameters: {'n_estimators': 1501, 'learning_rate': 0.0701848157354822, 'max_depth': 14, 'eval_metric': 'error'}. Best is trial 13 with value: 0.9746821153186992.\n",
            "[I 2024-02-17 22:05:26,069] Trial 30 finished with value: 0.9740713020953152 and parameters: {'n_estimators': 2414, 'learning_rate': 0.07858123725511705, 'max_depth': 15, 'eval_metric': 'error'}. Best is trial 13 with value: 0.9746821153186992.\n",
            "[I 2024-02-17 22:05:33,776] Trial 31 finished with value: 0.9745101807912562 and parameters: {'n_estimators': 2292, 'learning_rate': 0.06841086151671873, 'max_depth': 12, 'eval_metric': 'error'}. Best is trial 13 with value: 0.9746821153186992.\n",
            "[I 2024-02-17 22:05:42,318] Trial 32 finished with value: 0.9746021967469296 and parameters: {'n_estimators': 2422, 'learning_rate': 0.059907359027619254, 'max_depth': 14, 'eval_metric': 'error'}. Best is trial 13 with value: 0.9746821153186992.\n",
            "[I 2024-02-17 22:05:51,637] Trial 33 finished with value: 0.974362668855401 and parameters: {'n_estimators': 2427, 'learning_rate': 0.06044528910219626, 'max_depth': 15, 'eval_metric': 'error'}. Best is trial 13 with value: 0.9746821153186992.\n",
            "[I 2024-02-17 22:05:59,297] Trial 34 finished with value: 0.974175681039168 and parameters: {'n_estimators': 2058, 'learning_rate': 0.0665967699333305, 'max_depth': 14, 'eval_metric': 'error'}. Best is trial 13 with value: 0.9746821153186992.\n",
            "[I 2024-02-17 22:06:07,449] Trial 35 finished with value: 0.9741508977581422 and parameters: {'n_estimators': 2321, 'learning_rate': 0.08698412062606797, 'max_depth': 14, 'eval_metric': 'error'}. Best is trial 13 with value: 0.9746821153186992.\n",
            "[I 2024-02-17 22:06:15,524] Trial 36 finished with value: 0.974362668855401 and parameters: {'n_estimators': 2418, 'learning_rate': 0.09995933729860643, 'max_depth': 15, 'eval_metric': 'error'}. Best is trial 13 with value: 0.9746821153186992.\n",
            "[I 2024-02-17 22:06:19,188] Trial 37 finished with value: 0.9741894436449644 and parameters: {'n_estimators': 1974, 'learning_rate': 0.07496306569098615, 'max_depth': 5, 'eval_metric': 'error'}. Best is trial 13 with value: 0.9746821153186992.\n",
            "[I 2024-02-17 22:06:26,949] Trial 38 finished with value: 0.9740575813909806 and parameters: {'n_estimators': 1858, 'learning_rate': 0.05249752385104531, 'max_depth': 11, 'eval_metric': 'auc'}. Best is trial 13 with value: 0.9746821153186992.\n",
            "[I 2024-02-17 22:06:34,179] Trial 39 finished with value: 0.9737532923305009 and parameters: {'n_estimators': 2097, 'learning_rate': 0.08019463234460883, 'max_depth': 9, 'eval_metric': 'error'}. Best is trial 13 with value: 0.9746821153186992.\n",
            "[I 2024-02-17 22:06:38,244] Trial 40 finished with value: 0.9675108213746841 and parameters: {'n_estimators': 2448, 'learning_rate': 0.046276966716408206, 'max_depth': 3, 'eval_metric': 'error'}. Best is trial 13 with value: 0.9746821153186992.\n",
            "[I 2024-02-17 22:06:46,326] Trial 41 finished with value: 0.9736275458869015 and parameters: {'n_estimators': 2221, 'learning_rate': 0.05436809050373524, 'max_depth': 13, 'eval_metric': 'logloss'}. Best is trial 13 with value: 0.9746821153186992.\n",
            "[I 2024-02-17 22:06:54,949] Trial 42 finished with value: 0.9743763058097367 and parameters: {'n_estimators': 2303, 'learning_rate': 0.06015238886207631, 'max_depth': 15, 'eval_metric': 'auc'}. Best is trial 13 with value: 0.9746821153186992.\n",
            "[I 2024-02-17 22:07:02,649] Trial 43 finished with value: 0.9740438272869507 and parameters: {'n_estimators': 2389, 'learning_rate': 0.07267869796169564, 'max_depth': 13, 'eval_metric': 'logloss'}. Best is trial 13 with value: 0.9746821153186992.\n",
            "[I 2024-02-17 22:07:09,777] Trial 44 finished with value: 0.9740713020953152 and parameters: {'n_estimators': 2236, 'learning_rate': 0.05348790791664555, 'max_depth': 12, 'eval_metric': 'error'}. Best is trial 13 with value: 0.9746821153186992.\n",
            "[I 2024-02-17 22:07:17,381] Trial 45 finished with value: 0.9746829931248747 and parameters: {'n_estimators': 2450, 'learning_rate': 0.06307815778633971, 'max_depth': 10, 'eval_metric': 'logloss'}. Best is trial 45 with value: 0.9746829931248747.\n",
            "[I 2024-02-17 22:07:26,816] Trial 46 finished with value: 0.9746963449514564 and parameters: {'n_estimators': 2462, 'learning_rate': 0.0661010617284239, 'max_depth': 10, 'eval_metric': 'auc'}. Best is trial 46 with value: 0.9746963449514564.\n",
            "[I 2024-02-17 22:07:37,719] Trial 47 finished with value: 0.9748954912377273 and parameters: {'n_estimators': 2468, 'learning_rate': 0.0651822530918371, 'max_depth': 8, 'eval_metric': 'auc'}. Best is trial 47 with value: 0.9748954912377273.\n",
            "[I 2024-02-17 22:07:45,236] Trial 48 finished with value: 0.9745101807912562 and parameters: {'n_estimators': 2491, 'learning_rate': 0.06557903646423247, 'max_depth': 8, 'eval_metric': 'auc'}. Best is trial 47 with value: 0.9748954912377273.\n",
            "[I 2024-02-17 22:07:53,402] Trial 49 finished with value: 0.9747760564570367 and parameters: {'n_estimators': 2449, 'learning_rate': 0.061867454167375095, 'max_depth': 8, 'eval_metric': 'auc'}. Best is trial 47 with value: 0.9748954912377273.\n",
            "[I 2024-02-17 22:08:00,208] Trial 50 finished with value: 0.9747893465806128 and parameters: {'n_estimators': 2476, 'learning_rate': 0.07772930882770751, 'max_depth': 8, 'eval_metric': 'auc'}. Best is trial 47 with value: 0.9748954912377273.\n",
            "[I 2024-02-17 22:08:07,867] Trial 51 finished with value: 0.9741645561240189 and parameters: {'n_estimators': 2458, 'learning_rate': 0.0764016803639907, 'max_depth': 8, 'eval_metric': 'auc'}. Best is trial 47 with value: 0.9748954912377273.\n",
            "[I 2024-02-17 22:08:18,750] Trial 52 finished with value: 0.9752682275927347 and parameters: {'n_estimators': 2456, 'learning_rate': 0.08433865198013414, 'max_depth': 7, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:08:29,365] Trial 53 finished with value: 0.9737394560374629 and parameters: {'n_estimators': 2467, 'learning_rate': 0.08218772199431254, 'max_depth': 7, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:08:38,036] Trial 54 finished with value: 0.9746566203092972 and parameters: {'n_estimators': 2349, 'learning_rate': 0.08822436089789132, 'max_depth': 7, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:08:48,406] Trial 55 finished with value: 0.9744574347158228 and parameters: {'n_estimators': 2443, 'learning_rate': 0.09266166711131148, 'max_depth': 10, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:08:54,920] Trial 56 finished with value: 0.9746963449514564 and parameters: {'n_estimators': 2381, 'learning_rate': 0.08490320848460084, 'max_depth': 6, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:09:00,580] Trial 57 finished with value: 0.9739396800141163 and parameters: {'n_estimators': 2374, 'learning_rate': 0.08724590573348544, 'max_depth': 6, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:09:07,869] Trial 58 finished with value: 0.9744696553807323 and parameters: {'n_estimators': 2331, 'learning_rate': 0.08496782208399052, 'max_depth': 8, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:09:12,884] Trial 59 finished with value: 0.9745370341501114 and parameters: {'n_estimators': 2013, 'learning_rate': 0.07799332851694642, 'max_depth': 6, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:09:18,006] Trial 60 finished with value: 0.9740986437472047 and parameters: {'n_estimators': 2492, 'learning_rate': 0.07168469574822967, 'max_depth': 5, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:09:26,435] Trial 61 finished with value: 0.974577070973977 and parameters: {'n_estimators': 2395, 'learning_rate': 0.08188375929618831, 'max_depth': 9, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:09:33,420] Trial 62 finished with value: 0.9746300196163914 and parameters: {'n_estimators': 2456, 'learning_rate': 0.06259797297284687, 'max_depth': 7, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:09:43,301] Trial 63 finished with value: 0.9741917735962148 and parameters: {'n_estimators': 2374, 'learning_rate': 0.09016329160763407, 'max_depth': 9, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:09:52,659] Trial 64 finished with value: 0.9746300196163914 and parameters: {'n_estimators': 2500, 'learning_rate': 0.06706627651978894, 'max_depth': 8, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:10:02,524] Trial 65 finished with value: 0.9745108687903274 and parameters: {'n_estimators': 2439, 'learning_rate': 0.09525804586158965, 'max_depth': 10, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:10:07,364] Trial 66 finished with value: 0.9739396800141163 and parameters: {'n_estimators': 1691, 'learning_rate': 0.08424500401614991, 'max_depth': 7, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:10:15,446] Trial 67 finished with value: 0.9751084500115025 and parameters: {'n_estimators': 2470, 'learning_rate': 0.06400800695790346, 'max_depth': 9, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:10:23,462] Trial 68 finished with value: 0.9743509025617152 and parameters: {'n_estimators': 2403, 'learning_rate': 0.07394744704444244, 'max_depth': 9, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:10:30,340] Trial 69 finished with value: 0.9741645561240189 and parameters: {'n_estimators': 2356, 'learning_rate': 0.07045657339792433, 'max_depth': 8, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:10:36,243] Trial 70 finished with value: 0.9742188594505418 and parameters: {'n_estimators': 2474, 'learning_rate': 0.07585862083161758, 'max_depth': 6, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:10:44,916] Trial 71 finished with value: 0.9741529310925965 and parameters: {'n_estimators': 2433, 'learning_rate': 0.06302855634311921, 'max_depth': 10, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:10:53,015] Trial 72 finished with value: 0.9746427424628967 and parameters: {'n_estimators': 2465, 'learning_rate': 0.06552353926498143, 'max_depth': 9, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:11:00,479] Trial 73 finished with value: 0.9749756183767886 and parameters: {'n_estimators': 2409, 'learning_rate': 0.05844473032822687, 'max_depth': 8, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:11:07,349] Trial 74 finished with value: 0.9748291872652396 and parameters: {'n_estimators': 2264, 'learning_rate': 0.05719743713460238, 'max_depth': 8, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:11:14,460] Trial 75 finished with value: 0.9740849895106543 and parameters: {'n_estimators': 2269, 'learning_rate': 0.05758338678732688, 'max_depth': 8, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:11:21,071] Trial 76 finished with value: 0.9737394560374629 and parameters: {'n_estimators': 2330, 'learning_rate': 0.06032080760909717, 'max_depth': 7, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:11:29,071] Trial 77 finished with value: 0.9737394560374629 and parameters: {'n_estimators': 2416, 'learning_rate': 0.055117130696975936, 'max_depth': 9, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:11:35,690] Trial 78 finished with value: 0.9741508977581422 and parameters: {'n_estimators': 1896, 'learning_rate': 0.0671186292614364, 'max_depth': 8, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:11:41,547] Trial 79 finished with value: 0.9750417873411976 and parameters: {'n_estimators': 2187, 'learning_rate': 0.06107698817961192, 'max_depth': 7, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:11:47,340] Trial 80 finished with value: 0.9742829019304714 and parameters: {'n_estimators': 2169, 'learning_rate': 0.061663634077365886, 'max_depth': 7, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:11:53,717] Trial 81 finished with value: 0.9740986437472047 and parameters: {'n_estimators': 2129, 'learning_rate': 0.05833350628637317, 'max_depth': 8, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:12:00,360] Trial 82 finished with value: 0.9749620222889646 and parameters: {'n_estimators': 2200, 'learning_rate': 0.05583008992149004, 'max_depth': 8, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:12:05,962] Trial 83 finished with value: 0.9748823444867104 and parameters: {'n_estimators': 2124, 'learning_rate': 0.05588658640922207, 'max_depth': 7, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:12:12,647] Trial 84 finished with value: 0.9744560810420507 and parameters: {'n_estimators': 2196, 'learning_rate': 0.05051226721875281, 'max_depth': 7, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:12:18,229] Trial 85 finished with value: 0.974362668855401 and parameters: {'n_estimators': 2088, 'learning_rate': 0.04745837471341772, 'max_depth': 7, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:12:24,241] Trial 86 finished with value: 0.9742305308483741 and parameters: {'n_estimators': 2035, 'learning_rate': 0.05639903982948532, 'max_depth': 8, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:12:29,463] Trial 87 finished with value: 0.9749090330440523 and parameters: {'n_estimators': 2133, 'learning_rate': 0.05189338595020078, 'max_depth': 6, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:12:34,342] Trial 88 finished with value: 0.9729912412589119 and parameters: {'n_estimators': 2193, 'learning_rate': 0.05357734789477838, 'max_depth': 5, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:12:37,787] Trial 89 finished with value: 0.970121070647599 and parameters: {'n_estimators': 2127, 'learning_rate': 0.04896456147425127, 'max_depth': 4, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:12:42,856] Trial 90 finished with value: 0.9748158295450614 and parameters: {'n_estimators': 2259, 'learning_rate': 0.052677633194012405, 'max_depth': 6, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:12:47,920] Trial 91 finished with value: 0.9746292601536302 and parameters: {'n_estimators': 2217, 'learning_rate': 0.052877449517433096, 'max_depth': 6, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:12:53,839] Trial 92 finished with value: 0.9745629507412116 and parameters: {'n_estimators': 2257, 'learning_rate': 0.051150811247909406, 'max_depth': 7, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:12:58,103] Trial 93 finished with value: 0.9731433196352486 and parameters: {'n_estimators': 2153, 'learning_rate': 0.04550371656021026, 'max_depth': 5, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:13:03,474] Trial 94 finished with value: 0.9749889166272259 and parameters: {'n_estimators': 2174, 'learning_rate': 0.055544768699229016, 'max_depth': 6, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:13:09,056] Trial 95 finished with value: 0.9737912156060742 and parameters: {'n_estimators': 2112, 'learning_rate': 0.055690277198642786, 'max_depth': 7, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:13:14,314] Trial 96 finished with value: 0.9742692022205449 and parameters: {'n_estimators': 2170, 'learning_rate': 0.05817513414893109, 'max_depth': 6, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:13:20,100] Trial 97 finished with value: 0.9745358940658373 and parameters: {'n_estimators': 2218, 'learning_rate': 0.05937648147610789, 'max_depth': 7, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:13:25,572] Trial 98 finished with value: 0.9741372061591294 and parameters: {'n_estimators': 2293, 'learning_rate': 0.0550155467004044, 'max_depth': 6, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n",
            "[I 2024-02-17 22:13:32,462] Trial 99 finished with value: 0.9736837735343242 and parameters: {'n_estimators': 2075, 'learning_rate': 0.061233092702268406, 'max_depth': 9, 'eval_metric': 'auc'}. Best is trial 52 with value: 0.9752682275927347.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best trial: score 0.9752682275927347, \n",
            "params {'n_estimators': 2456, 'learning_rate': 0.08433865198013414, 'max_depth': 7, 'eval_metric': 'auc'}\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def objectiveXGB(trial, x_tr, y_tr, x_val, y_val):\n",
        "    param = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 1500, 2500),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.03, 0.1),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "        'objective': 'binary:logistic',  # 이진 분류\n",
        "        'eval_metric': trial.suggest_categorical(\"eval_metric\", [\"logloss\", \"auc\", \"error\"]),\n",
        "        'random_state': 0\n",
        "    }\n",
        "    \n",
        "    model = XGBClassifier(**param)\n",
        "    model.fit(x_tr, y_tr)\n",
        "    pred = model.predict(x_val)\n",
        "    score = f1_score(y_val, pred, average=\"weighted\")\n",
        "    \n",
        "    return score\n",
        "\n",
        "\n",
        "# 하이퍼 파라미터 튜닝\n",
        "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=0))\n",
        "study.optimize(lambda trial: objectiveXGB(trial, x_train, y_train, x_val, y_val), n_trials=100)\n",
        "\n",
        "print('Best trial: score {}, \\nparams {}'.format(study.best_trial.value, study.best_trial.params))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf1fcab3",
      "metadata": {},
      "source": [
        "Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2630cbaa",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-02-17 17:24:29,037] A new study created in memory with name: no-name-a651a7a3-95ce-4a8a-84f8-6916ff40f76a\n",
            "[I 2024-02-17 17:24:29,257] Trial 0 finished with value: 0.9615427574699139 and parameters: {'max_depth': 30, 'min_samples_split': 37, 'min_samples_leaf': 13, 'criterion': 'gini'}. Best is trial 0 with value: 0.9615427574699139.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best trial: score 0.9615427574699139, \n",
            "params {'max_depth': 30, 'min_samples_split': 37, 'min_samples_leaf': 13, 'criterion': 'gini'}\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def objectiveDT(trial, x_tr, y_tr, x_val, y_val):\n",
        "    param = {\n",
        "        'max_depth': trial.suggest_int('max_depth', 5, 50),\n",
        "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 50),\n",
        "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
        "        'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy']),\n",
        "        'random_state': 0\n",
        "    }\n",
        "    \n",
        "    model = DecisionTreeClassifier(**param)\n",
        "    model.fit(x_tr, y_tr)\n",
        "    pred = model.predict(x_val)\n",
        "    score = f1_score(y_val, pred, average=\"weighted\")\n",
        "    \n",
        "    return score\n",
        "\n",
        "\n",
        "# 하이퍼 파라미터 튜닝\n",
        "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=0))\n",
        "study.optimize(lambda trial: objectiveDT(trial, x_train, y_train, x_val, y_val), n_trials=1)\n",
        "\n",
        "print('Best trial: score {}, \\nparams {}'.format(study.best_trial.value, study.best_trial.params))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2959a287",
      "metadata": {},
      "source": [
        "ExtraTrees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cefd4703",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-02-17 17:24:34,746] A new study created in memory with name: no-name-4aca2d4a-d2ce-4e08-b4e7-56f635543340\n",
            "[I 2024-02-17 17:24:48,227] Trial 0 finished with value: 0.922567475656384 and parameters: {'n_estimators': 729, 'max_depth': 28, 'min_samples_split': 13, 'min_samples_leaf': 6, 'criterion': 'entropy'}. Best is trial 0 with value: 0.922567475656384.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best trial: score 0.922567475656384, \n",
            "params {'n_estimators': 729, 'max_depth': 28, 'min_samples_split': 13, 'min_samples_leaf': 6, 'criterion': 'entropy'}\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def objectiveET(trial, x_tr, y_tr, x_val, y_val):\n",
        "    param = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 400, 1000),\n",
        "        'max_depth': trial.suggest_int('max_depth', 10, 35),\n",
        "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
        "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
        "        'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy']),\n",
        "        'random_state': 0\n",
        "    }\n",
        "    \n",
        "    model = ExtraTreesClassifier(**param)\n",
        "    model.fit(x_tr, y_tr)\n",
        "    pred = model.predict(x_val)\n",
        "    score = f1_score(y_val, pred, average=\"weighted\")\n",
        "    \n",
        "    return score\n",
        "\n",
        "\n",
        "# 하이퍼 파라미터 튜닝\n",
        "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=0))\n",
        "study.optimize(lambda trial: objectiveET(trial, x_train, y_train, x_val, y_val), n_trials=1)\n",
        "\n",
        "print('Best trial: score {}, \\nparams {}'.format(study.best_trial.value, study.best_trial.params))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1baf6cfe",
      "metadata": {},
      "source": [
        "GradientBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ca1019e",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-02-17 17:26:44,855] A new study created in memory with name: no-name-a1d289c3-67d6-4220-b852-49ba548f1867\n",
            "[I 2024-02-17 17:30:06,455] Trial 0 finished with value: 0.9773593142990754 and parameters: {'n_estimators': 1139, 'learning_rate': 0.07436704297351776, 'max_depth': 10, 'min_samples_leaf': 11}. Best is trial 0 with value: 0.9773593142990754.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best trial: score 0.9773593142990754, \n",
            "params {'n_estimators': 1139, 'learning_rate': 0.07436704297351776, 'max_depth': 10, 'min_samples_leaf': 11}\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def objectiveGB(trial, x_tr, y_tr, x_val, y_val):\n",
        "    param = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 700, 1500),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
        "        'max_depth': trial.suggest_int('max_depth', 2, 15),\n",
        "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
        "        'random_state': 0\n",
        "    }\n",
        "    \n",
        "    model = GradientBoostingClassifier(**param)\n",
        "    model.fit(x_tr, y_tr)\n",
        "    pred = model.predict(x_val)\n",
        "    score = f1_score(y_val, pred, average=\"weighted\")\n",
        "    \n",
        "    return score\n",
        "\n",
        "\n",
        "# 하이퍼 파라미터 튜닝\n",
        "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=0))\n",
        "study.optimize(lambda trial: objectiveGB(trial, x_train, y_train, x_val, y_val), n_trials=1)\n",
        "\n",
        "print('Best trial: score {}, \\nparams {}'.format(study.best_trial.value, study.best_trial.params))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40c1c554",
      "metadata": {},
      "source": [
        "AdaBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "b4ea58a2",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-02-18 14:18:46,854] A new study created in memory with name: no-name-8e7f0449-1032-442b-8368-47e588a750a5\n",
            "[I 2024-02-18 14:19:48,953] Trial 0 finished with value: 0.9606806529165666 and parameters: {'n_estimators': 1549, 'learning_rate': 0.7436704297351775, 'algorithm': 'SAMME.R'}. Best is trial 0 with value: 0.9606806529165666.\n",
            "[I 2024-02-18 14:20:52,378] Trial 1 finished with value: 0.9599116042694162 and parameters: {'n_estimators': 1603, 'learning_rate': 0.5903948646972071, 'algorithm': 'SAMME.R'}. Best is trial 0 with value: 0.9606806529165666.\n",
            "[I 2024-02-18 14:21:48,775] Trial 2 finished with value: 0.9609638384546466 and parameters: {'n_estimators': 1424, 'learning_rate': 0.6813047017599905, 'algorithm': 'SAMME.R'}. Best is trial 2 with value: 0.9609638384546466.\n",
            "[I 2024-02-18 14:22:38,742] Trial 3 finished with value: 0.9616384187385358 and parameters: {'n_estimators': 1438, 'learning_rate': 0.9025957007038717, 'algorithm': 'SAMME.R'}. Best is trial 3 with value: 0.9616384187385358.\n",
            "[I 2024-02-18 14:23:40,441] Trial 4 finished with value: 0.9599174137177819 and parameters: {'n_estimators': 1964, 'learning_rate': 0.4450973669431999, 'algorithm': 'SAMME.R'}. Best is trial 3 with value: 0.9616384187385358.\n",
            "[I 2024-02-18 14:24:49,876] Trial 5 finished with value: 0.9601425745617207 and parameters: {'n_estimators': 1792, 'learning_rate': 0.5760054277776141, 'algorithm': 'SAMME.R'}. Best is trial 3 with value: 0.9616384187385358.\n",
            "[I 2024-02-18 14:25:53,999] Trial 6 finished with value: 0.9617858396745196 and parameters: {'n_estimators': 1568, 'learning_rate': 0.933036974463395, 'algorithm': 'SAMME.R'}. Best is trial 6 with value: 0.9617858396745196.\n",
            "[I 2024-02-18 14:26:37,790] Trial 7 finished with value: 0.9570466228444244 and parameters: {'n_estimators': 1071, 'learning_rate': 0.17841636973138664, 'algorithm': 'SAMME.R'}. Best is trial 6 with value: 0.9617858396745196.\n",
            "[I 2024-02-18 14:27:19,655] Trial 8 finished with value: 0.9603279947840184 and parameters: {'n_estimators': 1020, 'learning_rate': 0.8493578609931441, 'algorithm': 'SAMME.R'}. Best is trial 6 with value: 0.9617858396745196.\n",
            "[I 2024-02-18 14:28:32,806] Trial 9 finished with value: 0.9619423690370822 and parameters: {'n_estimators': 1778, 'learning_rate': 0.8830109334221372, 'algorithm': 'SAMME.R'}. Best is trial 9 with value: 0.9619423690370822.\n",
            "[I 2024-02-18 14:29:53,705] Trial 10 finished with value: 0.9592410800834336 and parameters: {'n_estimators': 1959, 'learning_rate': 0.36681826598977213, 'algorithm': 'SAMME.R'}. Best is trial 9 with value: 0.9619423690370822.\n",
            "[I 2024-02-18 14:30:54,444] Trial 11 finished with value: 0.9618116335922756 and parameters: {'n_estimators': 1755, 'learning_rate': 0.9606360070296472, 'algorithm': 'SAMME.R'}. Best is trial 9 with value: 0.9619423690370822.\n",
            "[I 2024-02-18 14:31:50,697] Trial 12 finished with value: 0.9626375971388691 and parameters: {'n_estimators': 1785, 'learning_rate': 0.9926812922100497, 'algorithm': 'SAMME.R'}. Best is trial 12 with value: 0.9626375971388691.\n",
            "[I 2024-02-18 14:32:48,688] Trial 13 finished with value: 0.9625707960935113 and parameters: {'n_estimators': 1771, 'learning_rate': 0.9881856103349944, 'algorithm': 'SAMME.R'}. Best is trial 12 with value: 0.9626375971388691.\n",
            "[I 2024-02-18 14:33:52,257] Trial 14 finished with value: 0.9617254065321518 and parameters: {'n_estimators': 1686, 'learning_rate': 0.7653956892456084, 'algorithm': 'SAMME.R'}. Best is trial 12 with value: 0.9626375971388691.\n",
            "[I 2024-02-18 14:34:41,095] Trial 15 finished with value: 0.9619423690370822 and parameters: {'n_estimators': 1295, 'learning_rate': 0.9908241812535508, 'algorithm': 'SAMME.R'}. Best is trial 12 with value: 0.9626375971388691.\n",
            "[I 2024-02-18 14:35:50,591] Trial 16 finished with value: 0.9622786805698299 and parameters: {'n_estimators': 1825, 'learning_rate': 0.8169334693275386, 'algorithm': 'SAMME.R'}. Best is trial 12 with value: 0.9626375971388691.\n",
            "[I 2024-02-18 14:36:56,582] Trial 17 finished with value: 0.9587941473533591 and parameters: {'n_estimators': 1895, 'learning_rate': 0.17803631754682753, 'algorithm': 'SAMME.R'}. Best is trial 12 with value: 0.9626375971388691.\n",
            "[I 2024-02-18 14:37:44,832] Trial 18 finished with value: 0.9613560973178662 and parameters: {'n_estimators': 1665, 'learning_rate': 0.6966200846102595, 'algorithm': 'SAMME.R'}. Best is trial 12 with value: 0.9626375971388691.\n",
            "[I 2024-02-18 14:38:22,622] Trial 19 finished with value: 0.9589018381789776 and parameters: {'n_estimators': 1219, 'learning_rate': 0.4077161969428186, 'algorithm': 'SAMME.R'}. Best is trial 12 with value: 0.9626375971388691.\n",
            "[I 2024-02-18 14:39:17,596] Trial 20 finished with value: 0.9623806089494013 and parameters: {'n_estimators': 1873, 'learning_rate': 0.9837528346766713, 'algorithm': 'SAMME.R'}. Best is trial 12 with value: 0.9626375971388691.\n",
            "[I 2024-02-18 14:40:14,607] Trial 21 finished with value: 0.9626818458092081 and parameters: {'n_estimators': 1850, 'learning_rate': 0.9895750747388307, 'algorithm': 'SAMME.R'}. Best is trial 21 with value: 0.9626818458092081.\n",
            "[I 2024-02-18 14:41:07,602] Trial 22 finished with value: 0.9622230110507617 and parameters: {'n_estimators': 1697, 'learning_rate': 0.8177773160959454, 'algorithm': 'SAMME.R'}. Best is trial 21 with value: 0.9626818458092081.\n",
            "[I 2024-02-18 14:42:08,004] Trial 23 finished with value: 0.9618218068107022 and parameters: {'n_estimators': 1982, 'learning_rate': 0.9061845350076304, 'algorithm': 'SAMME.R'}. Best is trial 21 with value: 0.9626818458092081.\n",
            "[I 2024-02-18 14:43:05,277] Trial 24 finished with value: 0.9627601092239423 and parameters: {'n_estimators': 1867, 'learning_rate': 0.9989367661990058, 'algorithm': 'SAMME.R'}. Best is trial 24 with value: 0.9627601092239423.\n",
            "[I 2024-02-18 14:44:02,499] Trial 25 finished with value: 0.9610356152157868 and parameters: {'n_estimators': 1876, 'learning_rate': 0.6486714846349407, 'algorithm': 'SAMME.R'}. Best is trial 24 with value: 0.9627601092239423.\n",
            "[I 2024-02-18 14:45:00,626] Trial 26 finished with value: 0.9620206924940476 and parameters: {'n_estimators': 1887, 'learning_rate': 0.8158118267727369, 'algorithm': 'SAMME.R'}. Best is trial 24 with value: 0.9627601092239423.\n",
            "[I 2024-02-18 14:45:50,221] Trial 27 finished with value: 0.9584308413951818 and parameters: {'n_estimators': 1631, 'learning_rate': 0.28354360063050355, 'algorithm': 'SAMME.R'}. Best is trial 24 with value: 0.9627601092239423.\n",
            "[I 2024-02-18 14:46:34,981] Trial 28 finished with value: 0.9623798348818107 and parameters: {'n_estimators': 1478, 'learning_rate': 0.9092932575702437, 'algorithm': 'SAMME.R'}. Best is trial 24 with value: 0.9627601092239423.\n",
            "[I 2024-02-18 14:47:28,264] Trial 29 finished with value: 0.9611445614247641 and parameters: {'n_estimators': 1726, 'learning_rate': 0.7266640982997103, 'algorithm': 'SAMME.R'}. Best is trial 24 with value: 0.9627601092239423.\n",
            "[W 2024-02-18 14:47:55,429] Trial 30 failed with parameters: {'n_estimators': 1531, 'learning_rate': 0.7630870873835812, 'algorithm': 'SAMME.R'} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"C:\\Users\\HamIG\\AppData\\Local\\Temp\\ipykernel_34908\\2412216491.py\", line 24, in <lambda>\n",
            "    study.optimize(lambda trial: objectiveAdaBoost(trial, x_train, y_train, x_val, y_val), n_trials=80)\n",
            "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\HamIG\\AppData\\Local\\Temp\\ipykernel_34908\\2412216491.py\", line 15, in objectiveAdaBoost\n",
            "    model.fit(x_tr, y_tr)\n",
            "  File \"c:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1351, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 169, in fit\n",
            "    sample_weight, estimator_weight, estimator_error = self._boost(\n",
            "                                                       ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 586, in _boost\n",
            "    return self._boost_real(iboost, X, y, sample_weight, random_state)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\", line 597, in _boost_real\n",
            "    estimator.fit(X, y, sample_weight=sample_weight)\n",
            "  File \"c:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1351, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1009, in fit\n",
            "    super()._fit(\n",
            "  File \"c:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 472, in _fit\n",
            "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
            "KeyboardInterrupt\n",
            "[W 2024-02-18 14:47:55,434] Trial 30 failed with value None.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[15], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# 하이퍼 파라미터 튜닝\u001b[39;00m\n\u001b[0;32m     23\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m, sampler\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTPESampler(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m---> 24\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjectiveAdaBoost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest trial: score \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mparams \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mvalue, study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mparams))\n",
            "File \u001b[1;32mc:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
            "File \u001b[1;32mc:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
            "File \u001b[1;32mc:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
            "Cell \u001b[1;32mIn[15], line 24\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# 하이퍼 파라미터 튜닝\u001b[39;00m\n\u001b[0;32m     23\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m, sampler\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTPESampler(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m---> 24\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[43mobjectiveAdaBoost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest trial: score \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mparams \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mvalue, study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mparams))\n",
            "Cell \u001b[1;32mIn[15], line 15\u001b[0m, in \u001b[0;36mobjectiveAdaBoost\u001b[1;34m(trial, x_tr, y_tr, x_val, y_val)\u001b[0m\n\u001b[0;32m      7\u001b[0m param \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m2000\u001b[39m),\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_float(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m1.0\u001b[39m),\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malgorithm\u001b[39m\u001b[38;5;124m'\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_categorical(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malgorithm\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSAMME.R\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     12\u001b[0m }\n\u001b[0;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m AdaBoostClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparam)\n\u001b[1;32m---> 15\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_val)\n\u001b[0;32m     17\u001b[0m score \u001b[38;5;241m=\u001b[39m f1_score(y_val, pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1349\u001b[0m     )\n\u001b[0;32m   1350\u001b[0m ):\n\u001b[1;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:169\u001b[0m, in \u001b[0;36mBaseWeightBoosting.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    166\u001b[0m sample_weight[zero_weight_mask] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;66;03m# Boosting step\u001b[39;00m\n\u001b[1;32m--> 169\u001b[0m sample_weight, estimator_weight, estimator_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_boost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43miboost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# Early termination\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:586\u001b[0m, in \u001b[0;36mAdaBoostClassifier._boost\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Implement a single boost.\u001b[39;00m\n\u001b[0;32m    548\u001b[0m \n\u001b[0;32m    549\u001b[0m \u001b[38;5;124;03mPerform a single boost according to the real multi-class SAMME.R\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;124;03m    If None then boosting has terminated early.\u001b[39;00m\n\u001b[0;32m    584\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgorithm \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSAMME.R\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 586\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_boost_real\u001b[49m\u001b[43m(\u001b[49m\u001b[43miboost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# elif self.algorithm == \"SAMME\":\u001b[39;00m\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_boost_discrete(iboost, X, y, sample_weight, random_state)\n",
            "File \u001b[1;32mc:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:597\u001b[0m, in \u001b[0;36mAdaBoostClassifier._boost_real\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Implement a single boost using the SAMME.R real algorithm.\"\"\"\u001b[39;00m\n\u001b[0;32m    595\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m--> 597\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    599\u001b[0m y_predict_proba \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mpredict_proba(X)\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iboost \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1349\u001b[0m     )\n\u001b[0;32m   1350\u001b[0m ):\n\u001b[1;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1009\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    979\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    980\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    981\u001b[0m \n\u001b[0;32m    982\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1009\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1015\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\HamIG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def objectiveAdaBoost(trial, x_tr, y_tr, x_val, y_val):\n",
        "    param = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 1000, 2000),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.1, 1.0),\n",
        "        'algorithm': trial.suggest_categorical('algorithm', ['SAMME.R']),\n",
        "        'random_state': 0\n",
        "    }\n",
        "    \n",
        "    model = AdaBoostClassifier(**param)\n",
        "    model.fit(x_tr, y_tr)\n",
        "    pred = model.predict(x_val)\n",
        "    score = f1_score(y_val, pred, average=\"weighted\")\n",
        "    \n",
        "    return score\n",
        "\n",
        "\n",
        "# 하이퍼 파라미터 튜닝\n",
        "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=0))\n",
        "study.optimize(lambda trial: objectiveAdaBoost(trial, x_train, y_train, x_val, y_val), n_trials=80)\n",
        "\n",
        "print('Best trial: score {}, \\nparams {}'.format(study.best_trial.value, study.best_trial.params))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3401ec15",
      "metadata": {
        "id": "3401ec15"
      },
      "source": [
        "### 모델 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a019f4c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "# 보팅\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "# 스테킹\n",
        "from sklearn.ensemble import StackingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "9193fac9",
      "metadata": {
        "id": "9193fac9"
      },
      "outputs": [],
      "source": [
        "# RandomForest\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=1056     \n",
        "    , max_depth=30    \n",
        "    , min_samples_split=3   \n",
        "    , min_samples_leaf=1   \n",
        "    , bootstrap=True\n",
        "    , criterion='entropy'\n",
        ")\n",
        "\n",
        "# LightGBM\n",
        "lgb_model = LGBMClassifier(\n",
        "    num_leaves=162\n",
        "    , max_depth=10\n",
        "    , n_estimators=487\n",
        "    , learning_rate=0.07324658507873466\n",
        "    , min_child_samples=31\n",
        "    , verbose = -1\n",
        ")\n",
        "\n",
        "# LightGBM_dart\n",
        "lgb_dart_model = LGBMClassifier(\n",
        "    num_leaves=170\n",
        "    , max_depth=13\n",
        "    , n_estimators=692\n",
        "    , learning_rate=0.0916736042020453\n",
        "    , min_child_samples=10\n",
        "    , verbose = -1\n",
        "    , boosting_type=\"dart\"\n",
        ")\n",
        "\n",
        "# XGBoost \n",
        "xgb_model = XGBClassifier(\n",
        "    n_estimators=1427\n",
        "    , learning_rate=0.08645845446703926\n",
        "    , max_depth=7\n",
        "    , objective='binary:logistic'\n",
        "    , eval_metric = 'error'\n",
        ")\n",
        "\n",
        "# GradientBoosting\n",
        "gb_model = GradientBoostingClassifier(\n",
        "    n_estimators=1425\n",
        "    , learning_rate=0.09883679411048218\n",
        "    , max_depth=6\n",
        "    , min_samples_leaf=13\n",
        ")\n",
        "\n",
        "# DecisionTree\n",
        "dt_model = DecisionTreeClassifier(\n",
        "    max_depth=24\n",
        "    , min_samples_split=2  # 노드를 분할하기 위한 최소 샘플 수\n",
        "    , min_samples_leaf=1  # 리프 노드에 필요한 최소 샘플 수\n",
        "    , criterion = 'entropy'\n",
        ")  \n",
        "\n",
        "# ExtraTrees\n",
        "et_model = ExtraTreesClassifier(\n",
        "    n_estimators=486\n",
        "    , min_samples_split=3  # 노드를 분할하기 위한 최소 샘플 수\n",
        "    , min_samples_leaf=1   # 리프 노드에 필요한 최소 샘플 수\n",
        "    , max_depth=26 \n",
        "    , criterion = 'entropy'\n",
        ")  \n",
        "\n",
        "# AdaBoost\n",
        "ada_model = AdaBoostClassifier(\n",
        "    n_estimators=1399\n",
        "    , learning_rate=0.9987147599335517\n",
        "    , algorithm='SAMME.R'\n",
        ")  \n",
        "\n",
        "# CatBoost\n",
        "#cat_model = CatBoostClassifier(\n",
        "#    iterations=1045\n",
        "#   , learning_rate=0.21147352826666405\n",
        "#    , depth=9\n",
        "#    , verbose=False\n",
        "#)\n",
        "\n",
        "\n",
        "### 스태킹 분류기 생성 ###\n",
        "model = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('rf', rf_model),\n",
        "        ('lgb', lgb_model),\n",
        "        ('xgb', xgb_model),\n",
        "        ('gb', gb_model),\n",
        "        ('dt', dt_model),\n",
        "        ('lgb_dart',lgb_dart_model)\n",
        "        #('cat', cat_model)\n",
        "    ],\n",
        "    final_estimator=lgb_model  # 최종 메타 모델\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cacd5ed8",
      "metadata": {
        "id": "cacd5ed8"
      },
      "source": [
        "### 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "9df5f040",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "9df5f040",
        "outputId": "a8b64c9c-5378-4f4b-d555-1213436c7e99"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-3 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: black;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-3 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-3 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-3 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-3 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-3 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-3 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-3 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-3 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-3 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-3 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 1ex;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-3 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-3 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StackingClassifier(estimators=[(&#x27;rf&#x27;,\n",
              "                                RandomForestClassifier(criterion=&#x27;entropy&#x27;,\n",
              "                                                       max_depth=30,\n",
              "                                                       min_samples_split=3,\n",
              "                                                       n_estimators=1056)),\n",
              "                               (&#x27;lgb&#x27;,\n",
              "                                LGBMClassifier(learning_rate=0.07324658507873466,\n",
              "                                               max_depth=10,\n",
              "                                               min_child_samples=31,\n",
              "                                               n_estimators=487, num_leaves=162,\n",
              "                                               verbose=-1)),\n",
              "                               (&#x27;xgb&#x27;,\n",
              "                                XGBClassifier(base_score=None, booster=None,\n",
              "                                              callbacks=None,\n",
              "                                              colsample_bylevel...\n",
              "                                DecisionTreeClassifier(criterion=&#x27;entropy&#x27;,\n",
              "                                                       max_depth=24)),\n",
              "                               (&#x27;lgb_dart&#x27;,\n",
              "                                LGBMClassifier(boosting_type=&#x27;dart&#x27;,\n",
              "                                               learning_rate=0.0916736042020453,\n",
              "                                               max_depth=13,\n",
              "                                               min_child_samples=10,\n",
              "                                               n_estimators=692, num_leaves=170,\n",
              "                                               verbose=-1))],\n",
              "                   final_estimator=LGBMClassifier(boosting_type=&#x27;dart&#x27;,\n",
              "                                                  learning_rate=0.0916736042020453,\n",
              "                                                  max_depth=13,\n",
              "                                                  min_child_samples=10,\n",
              "                                                  n_estimators=692,\n",
              "                                                  num_leaves=170, verbose=-1))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;StackingClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.StackingClassifier.html\">?<span>Documentation for StackingClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>StackingClassifier(estimators=[(&#x27;rf&#x27;,\n",
              "                                RandomForestClassifier(criterion=&#x27;entropy&#x27;,\n",
              "                                                       max_depth=30,\n",
              "                                                       min_samples_split=3,\n",
              "                                                       n_estimators=1056)),\n",
              "                               (&#x27;lgb&#x27;,\n",
              "                                LGBMClassifier(learning_rate=0.07324658507873466,\n",
              "                                               max_depth=10,\n",
              "                                               min_child_samples=31,\n",
              "                                               n_estimators=487, num_leaves=162,\n",
              "                                               verbose=-1)),\n",
              "                               (&#x27;xgb&#x27;,\n",
              "                                XGBClassifier(base_score=None, booster=None,\n",
              "                                              callbacks=None,\n",
              "                                              colsample_bylevel...\n",
              "                                DecisionTreeClassifier(criterion=&#x27;entropy&#x27;,\n",
              "                                                       max_depth=24)),\n",
              "                               (&#x27;lgb_dart&#x27;,\n",
              "                                LGBMClassifier(boosting_type=&#x27;dart&#x27;,\n",
              "                                               learning_rate=0.0916736042020453,\n",
              "                                               max_depth=13,\n",
              "                                               min_child_samples=10,\n",
              "                                               n_estimators=692, num_leaves=170,\n",
              "                                               verbose=-1))],\n",
              "                   final_estimator=LGBMClassifier(boosting_type=&#x27;dart&#x27;,\n",
              "                                                  learning_rate=0.0916736042020453,\n",
              "                                                  max_depth=13,\n",
              "                                                  min_child_samples=10,\n",
              "                                                  n_estimators=692,\n",
              "                                                  num_leaves=170, verbose=-1))</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>rf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;RandomForestClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(criterion=&#x27;entropy&#x27;, max_depth=30, min_samples_split=3,\n",
              "                       n_estimators=1056)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>lgb</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">LGBMClassifier</label><div class=\"sk-toggleable__content fitted\"><pre>LGBMClassifier(learning_rate=0.07324658507873466, max_depth=10,\n",
              "               min_child_samples=31, n_estimators=487, num_leaves=162,\n",
              "               verbose=-1)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>xgb</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">XGBClassifier</label><div class=\"sk-toggleable__content fitted\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=&#x27;error&#x27;, feature_types=None,\n",
              "              gamma=None, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=0.08645845446703926,\n",
              "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=7, max_leaves=None,\n",
              "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "              multi_strategy=None, n_estimators=1427, n_jobs=None,\n",
              "              num_parallel_tree=None, random_state=None, ...)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>gb</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;GradientBoostingClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\">?<span>Documentation for GradientBoostingClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>GradientBoostingClassifier(learning_rate=0.09883679411048218, max_depth=6,\n",
              "                           min_samples_leaf=13, n_estimators=1425)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>dt</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;DecisionTreeClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">?<span>Documentation for DecisionTreeClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=24)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>lgb_dart</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">LGBMClassifier</label><div class=\"sk-toggleable__content fitted\"><pre>LGBMClassifier(boosting_type=&#x27;dart&#x27;, learning_rate=0.0916736042020453,\n",
              "               max_depth=13, min_child_samples=10, n_estimators=692,\n",
              "               num_leaves=170, verbose=-1)</pre></div> </div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>final_estimator</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" ><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">LGBMClassifier</label><div class=\"sk-toggleable__content fitted\"><pre>LGBMClassifier(boosting_type=&#x27;dart&#x27;, learning_rate=0.0916736042020453,\n",
              "               max_depth=13, min_child_samples=10, n_estimators=692,\n",
              "               num_leaves=170, verbose=-1)</pre></div> </div></div></div></div></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "StackingClassifier(estimators=[('rf',\n",
              "                                RandomForestClassifier(criterion='entropy',\n",
              "                                                       max_depth=30,\n",
              "                                                       min_samples_split=3,\n",
              "                                                       n_estimators=1056)),\n",
              "                               ('lgb',\n",
              "                                LGBMClassifier(learning_rate=0.07324658507873466,\n",
              "                                               max_depth=10,\n",
              "                                               min_child_samples=31,\n",
              "                                               n_estimators=487, num_leaves=162,\n",
              "                                               verbose=-1)),\n",
              "                               ('xgb',\n",
              "                                XGBClassifier(base_score=None, booster=None,\n",
              "                                              callbacks=None,\n",
              "                                              colsample_bylevel...\n",
              "                                DecisionTreeClassifier(criterion='entropy',\n",
              "                                                       max_depth=24)),\n",
              "                               ('lgb_dart',\n",
              "                                LGBMClassifier(boosting_type='dart',\n",
              "                                               learning_rate=0.0916736042020453,\n",
              "                                               max_depth=13,\n",
              "                                               min_child_samples=10,\n",
              "                                               n_estimators=692, num_leaves=170,\n",
              "                                               verbose=-1))],\n",
              "                   final_estimator=LGBMClassifier(boosting_type='dart',\n",
              "                                                  learning_rate=0.0916736042020453,\n",
              "                                                  max_depth=13,\n",
              "                                                  min_child_samples=10,\n",
              "                                                  n_estimators=692,\n",
              "                                                  num_leaves=170, verbose=-1))"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bf2de5f",
      "metadata": {
        "id": "6bf2de5f"
      },
      "source": [
        "### 모델 성능 보기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "c8871444",
      "metadata": {
        "id": "c8871444"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    confusion_matrix,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        ")\n",
        "\n",
        "def get_clf_eval(y_test, y_pred=None):\n",
        "    confusion = confusion_matrix(y_test, y_pred, labels=[True, False])\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, labels=[True, False])\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    F1 = f1_score(y_test, y_pred, labels=[True, False])\n",
        "    weighted_F1 = f1_score(y_test, y_pred, average='weighted')  # 추가된 부분 \n",
        "\n",
        "    metrics = pd.DataFrame({\n",
        "        '정확도': [accuracy],\n",
        "        '정밀도': [precision],\n",
        "        '재현율': [recall],\n",
        "        'F1 Score': [F1],\n",
        "        'Weighted F1': [weighted_F1]  # 추가된 부분\n",
        "    })\n",
        "\n",
        "    confusion_df = pd.DataFrame(confusion, index=['True', 'False'], columns=['True', 'False'])\n",
        "\n",
        "    print(\"\\n오차행렬:\")\n",
        "    display(confusion_df)\n",
        "    print(\"평가 지표:\")\n",
        "    display(metrics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "56a86373",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "56a86373",
        "outputId": "b0c0f95b-f5c0-4530-fdb2-a0ecc2b31b5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "오차행렬:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>True</th>\n",
              "      <th>False</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>True</th>\n",
              "      <td>756</td>\n",
              "      <td>191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>False</th>\n",
              "      <td>101</td>\n",
              "      <td>10812</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       True  False\n",
              "True    756    191\n",
              "False   101  10812"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "평가 지표:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>정확도</th>\n",
              "      <th>정밀도</th>\n",
              "      <th>재현율</th>\n",
              "      <th>F1 Score</th>\n",
              "      <th>Weighted F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.975379</td>\n",
              "      <td>0.882147</td>\n",
              "      <td>0.79831</td>\n",
              "      <td>0.838137</td>\n",
              "      <td>0.974816</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        정확도       정밀도      재현율  F1 Score  Weighted F1\n",
              "0  0.975379  0.882147  0.79831  0.838137     0.974816"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pred = model.predict(x_val)\n",
        "get_clf_eval(y_val, pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7adf8300",
      "metadata": {
        "id": "7adf8300"
      },
      "source": [
        "## 4. 제출하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d0b6e17",
      "metadata": {
        "id": "9d0b6e17"
      },
      "source": [
        "### 테스트 데이터 예측"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "43daa73c",
      "metadata": {
        "id": "43daa73c"
      },
      "outputs": [],
      "source": [
        "# 예측에 필요한 데이터 분리\n",
        "x_test = df_test_encoded.drop([\"is_converted\", \"id\"], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "d13f7a6e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d13f7a6e",
        "outputId": "dcef4162-f5dc-4624-a9d8-b997299b4591"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "625"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_pred = model.predict(x_test)\n",
        "sum(test_pred) # True로 예측된 개수"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47f18e6a",
      "metadata": {
        "id": "47f18e6a"
      },
      "source": [
        "### 제출 파일 작성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3128a458",
      "metadata": {
        "id": "3128a458"
      },
      "outputs": [],
      "source": [
        "# 제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
        "df_sub = pd.read_csv(\"./data/submission.csv\")\n",
        "df_sub[\"is_converted\"] = test_pred\n",
        "\n",
        "# 제출 파일 저장\n",
        "df_sub.to_csv(\"submission_model_12.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec7867ce",
      "metadata": {
        "id": "ec7867ce"
      },
      "source": [
        "**우측 상단의 제출 버튼을 클릭해 결과를 확인하세요**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "413b3cb9",
      "metadata": {},
      "source": [
        "."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
